---
title: "Bounded mean test - scribbles"
format: html
editor: source
---


## The problem

Given a sequence $X_1, X_2, \ldots$ of i.i.d. random variables with support on $[0, \infty)$ with the common distribution $X$ and CDF $F$. We want to:

a)  find a powerful test for the null $H_0 : E(X) \leq \mu$ against the alternative $H_1 : E(X) > \mu$ and/or
b)  compute a lower confidence bound on $E(X)$ with guaranteed coverage properties

While this is of interest by itself, we in the end want to combine two such one-sided tests to obtain a test for the mean of a variable over $[0,1]$ --- so if necessary, some extra assumptions can be made (e.g. all moments bounded).

## Augmented bootstrap test

The overall idea: mixture of the bootstrap distribution with a proportion of zeroes, such that the mean is $\mu$

Note on notation: we will index all variables derived from the observed sample by the number of draws observed ($n$). Whenever we use $X$ without any index, we implicitly mean that the claim is true for all $X_i$.

Denote $\bar{X}_n = \frac{\sum_{i=1}^n X_i}{n}$.

For a sample of size $n$ we never reject when $\bar{X}_n \leq \mu$. When $\bar{X}_n > \mu$ we define r.v. $A_n = 1 - \frac{\mu}{\bar{X}_n}$.^[Choosing $A$ in this way was suggested by an anonymous user on Cross Validated questions and answer site: https://stats.stackexchange.com/a/650747/73129] We construct $B_n$ as the mixture of a point mass at zero with weight $A_n$ and the bootstrap distribution with weight $1 - A_n$ and look at the distribution of the sample mean for those variables. More formally, we now consider the i.i.d random variables $Z_{n,1}, \ldots, Z_{n,n}$ with support $\{0, 1, \ldots, n\}$ and $B_{n,1}, \ldots, B_{n,n}$ over $[0, \infty)$. For all $i \in \{1, \ldots, n\}$ we have:

$$
\begin{align}
P(Z_{n,i} = 0) &= A_n \\
P(Z_{n,i} = k) &= \frac{1 - A_n}{n} = \frac{\mu}{n \bar{X}_n}  \text{ for } \ 0 < k \leq n \\
B_{n,i} &= \begin{cases} 0 & Z_{n,i} = 0 \\
X_{Z_{n,i}} & \text{otherwise} 
\end{cases}
\end{align}
$$

Note that $E(B_{n,i} | \bar{X}_n > \mu) = \mu$. Now we obtain the r.v. $S_n$

$$
S_n = \frac{1}{n}\sum_{i=1}^n B_{n,i}
$$

Note that given specific observations of $X_1, \ldots, X_n$ we can obtain samples of $S_n$ easily in a Monte Carlo fashion. We then reject at level $\alpha$ if the observed value of the mean is above the $1 - \alpha$ quantile of $S_n$. We now need to prove that this is a valid testing procedure.

$$
p_n(x_1, \ldots, x_n) = \begin{cases}
1 & \sum x_i \leq n\mu \\
P(S_n \geq \frac{1}{n}\sum x_i | X_1 = x_1, ..., X_n = x_n) & \text{otherwise}
\end{cases}
$$

$$
P(p(X_1, \ldots, X_n) < \alpha) < \alpha \\
P(E_B(\mathbb{I}(S_n \geq \bar{X}_n)) < \alpha \wedge \bar{X}_n > \mu) < \alpha
$$

for $n = 1$

\$\$ P(E_B(\mathbb{I}(B_1 \geq X_1)) \< \alpha \wedge X_1 \> \mu) = \\ P(E_Z(Z_1 = 1) \< \alpha \wedge X_1 \> \mu) = \\ P(\frac{\mu}{X_1} \< \alpha \wedge X_1 \> \mu) = \\ P(X_1 \> \frac{\mu}{\alpha} \wedge X_1 \> \mu) = \\ P(X_1 \> \frac{\mu}{\alpha})

\$\$

Since $E(X_1) = \mu$ and $E(X_1) > \frac{\mu}{\alpha} P(X_1 > \frac{\mu}{\alpha})$ we have

$$
P(X_1 > \frac{\mu}{\alpha})  < \frac{\mu}{\frac{\mu}{\alpha}} = \alpha
$$

for $n \to \infty$, we have $P(S_n \leq s | \bar{X}_n > \mu) \to P(\bar{X}_n \leq s | \bar{X}_n > \mu)$:

$$
\lim_{n \to \infty} P(E_B(\mathbb{I}(S_n \geq \bar{X}_n)) < \alpha \wedge \bar{X}_n > \mu) = \\
\lim_{n \to \infty} P(E_B(\mathbb{I}(S_n \geq \mu)) < \alpha | \bar{X}_n > \mu) P(\bar{X}_n > \mu) = \\
$$

The test can then be inverted to obtain cofindence bounds.


```{r}
N_sims <- 10000
K <- 5

# gen <- \(n) rbeta(n, 3, 1)
# mu <- 3/4

# gen <- \(n)  dplyr::if_else(runif(n) < 0.5, 0.1, 0.5 + 0.5 * rbeta(n, 2, 1))
# mu <- 0.5 * (0.1 + 0.5 + 1/3)
gen <- \(n)  dplyr::if_else(runif(n) < 0.5, 0.1, 0.5 + 0.5 * runif(n))
mu <- 0.5 * (0.1 + 0.75)
# pgen <- function(s) {
#   dplyr::case_when(s < 0.1 ~ 0,
#                    s < 0.5 ~ 0.5,
#                    TRUE ~ 0.5 + 0.5 * punif((s - 0.5) * 2))
# }

s <- seq(mu + 0.01, 0.99, length.out = 10)


X_matrix <- matrix(gen(N_sims * K), nrow = N_sims)

if(abs(mean(X_matrix) - mu) > 0.01) {
  stop("Suspicious mean")
} 

pB_over_s <- function(s, X) {
  if(mean(X) <= mu) {
    return(NA_real_)
  } else {
    p <- mu / sum(X)
    return(p * sum(X > s))    
  }
}

pB_array <- array(NA_real_, dim = c(N_sims, K, length(s)))

for(k in 1:K) {
  for(s_id in seq_along(s)) {
    pB_array[, k, s_id] <- apply(X_matrix[,1:k, drop = FALSE], MARGIN = 1, FUN = \(X) pB_over_s(s[s_id], X))
  }
}

pB_avg <- apply(pB_array, MARGIN = c(2,3), mean, na.rm = TRUE)
pB_N <- apply(pB_array, MARGIN = c(2,3), \(x) sum(!is.na(x)))
matplot(pB_avg, type = "l")

pX <- 1 - ecdf(X_matrix)(s)
#pX <- ecdf(-X_matrix)(-s)
#pX

pB_avg_rel <- sweep(pB_avg, MARGIN = 2, STATS = pX, FUN = "/")
matplot(pB_avg_rel, type = "l")
pB_avg_rel

xxx <- pB_array[,1,10]
hist(xxx)
# posterior::mcse_mean(xxx[!is.na(xxx)])
```


P(X \> 0.99) = 1/2 \* (2/100) = 1/100


```{r}
N_sims <- 10000
K <- 17

# gen <- \(n) rbeta(n, 3, 1)
# mu <- 3/4

# gen <- \(n)  dplyr::if_else(runif(n) < 0.5, 0.1, 0.5 + 0.5 * rbeta(n, 2, 1))
# mu <- 0.5 * (0.1 + 0.5 + 1/3)

gen <- \(n)  dplyr::if_else(runif(n) < 0.5, 0.1, 0.5 + 0.5 * runif(n))
mu <- 0.5 * (0.1 + 0.75)


s <- seq(mu + 0.01, 0.99, length.out = 10)


X_matrix <- matrix(gen(N_sims * K), nrow = N_sims)

if(abs(mean(X_matrix) - mu) > 0.01) {
  stop("Suspicious mean")
} 


pS_array <- array(NA_real_, c(K, N_sims, length(s)))

B <- 1000
for(k in 1:K) {
  for(sim in 1:N_sims) {
    probs <- X_matrix[sim, 1:k]
    s_probs <- sum(probs)
    n <- length(probs)
    m_probs <- s_probs/n
    if(m_probs <= mu) {
      pS_array[k, sim,] <- NA_real_
    } else {
      e_probs <- c(probs, 0)
      a <- 1 - mu / m_probs
      p_probs <- c(rep(mu / s_probs, n), a)
      bs_matrix <- matrix(nrow = B, ncol = n, sample(e_probs, prob = p_probs, size = B * n, replace = TRUE))
      bs <- rowMeans(bs_matrix)
      pS_array[k, sim, ] <- 1 - ecdf(bs)(s)
    }
  }
}

pS <- apply(pS_array, MARGIN = c(1,3), FUN = mean, na.rm = TRUE)

cc <- apply(X_matrix, MARGIN = 1, FUN = cumsum)
X_means <- sweep(
  apply(X_matrix, MARGIN = 1, FUN = cumsum),
  MARGIN = 1, STATS = 1:K, FUN = "/")

pX <- matrix(NA_real_, nrow = K, ncol = length(s))
for(s_id in seq_along(s)) {
  pX[,s_id] = rowMeans(X_means > s[s_id])
}

matplot(pS, type = "l")
matplot(pX, type = "l")

matplot(pS / pX, type = "l")

pS / pX
```


## Approximate computation

Computation time gains (especially for larger $n$) likely possible with a saddlepoint approximation to the bootstrap distribution (https://www.jstor.org/stable/2336592). This will be particularly useful to compute extreme tail probabilities and to invert the test should we want to obtain confidence bounds.

## Possible extensions:

The current proposal assumes the CDF of the distribution is upper bounded by a CDF of a point mass at 0. The method should however work by replacing the point mass with an arbitrary distribution with known mean as long as its CDF bounds the set of distributions we want to test. Instead of adding $0$ to the bootstrap distribution we would then add draws from this bounding distribution.
## Literature

**Summary:** it appears that numerically the best performing method for variables bounded from both sides is that of Gaffke 2005, which is more thoroughly investigated in Learned-Miller and Thomas 2019 [A new confidence interval for the mean of a bounded random variable](https://arxiv.org/pdf/1905.06208). However, this method currently has been proven to be valid only in special cases. The proposed method gives numerically similar results to Gaffke, possible even slightly tighter.

A possibly oldest reference is Anderson 1969: [Confidence limits for the expected value of an arbitrary bounded random variable with a continuous distribution function](https://purl.stanford.edu/xc975jg9300) - Via Diouf and doufour better than Hoeffding and Hora & Hora

Bickel 1992: [Inference and Auditing: The Stringer Bound](https://doi.org/10.2307/1403650), discusses Stringer bound (strictly upper bound) and offers improvements. Uses a point mass at 1 in a special case, so related to my proposal, also uses the empirical distribution (of non-zero values). Claim one can move to two-sided bound. -\> Need to Romano & Wolf claim that methods are not conservative and efficient.

Diouf & Dufour 2005: [Improved Nonparametric Inference for the Mean of a Bounded Random Variable with Application to Poverty Measures](http://web.hec.ca/scse/articles/Diouf.pdf), improve the Anderson 1969 method and provide an additional set of intervals based on confidence bands for the CDF based on eCDF. There does not appear to be huge variability between the methods.

Learned-Miller and Thomas 2019 [A new confidence interval for the mean of a bounded random variable](https://arxiv.org/pdf/1905.06208) Mix the ordered observations with an ordered vector of uniformly distributed random variables. Compute with Monte Carlo (easy). Looks very good! Phan & Learned-Miller claim this to be rediscovery of Gaffke 2005. No proof of guaranteed coverage, only conjecture and proof for some special cases. TODO go through citing papers

Gaffke, N. Three test statistics for a nonparametric one- sided hypothesis on the mean of a nonnegative variable. Mathematical Methods of Statistics, 14(4):451â€“467, 2005. Matches the alg in Learned-Miller & Thomas, less evaluation.

Phan & Thomas & Learned-Miller 2021: [Towards Practical Mean Bounds for Small Samples](http://proceedings.mlr.press/v139/phan21a/phan21a.pdf) Claim that Diouf & Dufour as well as Romano & Wolf do not dominate Anderson, while their technique does. Do not compare to Gaffke/Learned-Miller & Thomas 2019 - presumably because it isn't better? But the method shares similarities - combine with a random uniform vector... Looks computationally mildly expensive (solve a linear program for each bootstrap sample).

Romano & Wolf 2000: [Finite sample nonparametric inference and large sample efficiency](https://doi.org/10.1214/aos/1015951997) prove that there is an asymptotic upper bound on efficiency of any test in this setting and construct an (IMHO somewhat clunky) test that meets this asymptotic bound.

Waudby-Smith & Aaditya Ramdas 2022: [Estimating means of bounded random variables by betting](https://arxiv.org/pdf/2010.09686) give both confidence intervals and confidence sequences, but focuse on the case of distributions over just $\{0,1\}$. Their bounds are valid for general distributions on $[0,1]$, but do not exploit the presence of intermediate values and are thus unnecessarily conservative. Discussion by Li, Li & Dai does some numerical comparisons. Supplementary Fig 16 has comparison to Phan & Thomas & Learned-Miller 2021 (favorable to Waudby-Smith). Basically admit Gafke/Learned-Miller and Thomas 2019 are much better.

Schlag's thesis [New Method for Constructing Exact Tests without Making any Assumptions](https://econ-papers.upf.edu/papers/1109.pdf) Numerical simulations lacking, but seems that with n = 60 still has about 1.5 times wider CI than asymptotic (Table 1). There's also a working paper Schlag 2013 [Exact Hypothesis Testing without Assumptions -New and Old Results not only for Experimental Game Theory](https://homepage.univie.ac.at/karl.schlag/research/statistics/exacthypothesistesting8.pdf) Repeatedly randomize to {0,1} and use a test on binary outcomes, then average. Just a test, no CI (But can be inverted). Implemented in the `npExact` R package.

Austern & Mackey 2024: [Efficient Concentration with Gaussian Approximation](https://arxiv.org/pdf/2208.09922). Close to asymptotic performance, but use pretty large n in experiments. Claim to outperform Romano & Wolf 202.

Orabona & Jun 2022, [Tight Concentrations and Confidence Sequences from the Regret of Universal Portfolio](https://arxiv.org/abs/2208.09922) A new concentration inequality, call it PRECiSE-A-CO96. Seems like minor improvement over Waudyb-Smith. Code in Matlab :-(. Require N \> 10000 to come close to asymptotic.

Fienberg, S. E., Neter, J., and Leitch, R. A. Estimating the total overstatement error in accounting populations. Journal of the American Statistical Association, 72(358): 295â€“302, 1977. Phan & Thomas & Learned-Miller 2021 claim this is nice but computationally intractable Estimates multinomial uncertainty for bins of (rounded) values and then combines those together to form confidence for the mean.

Wang, W., & Zhao, L. H. (2003). [Nonparametric tests for the mean of a non-negative population.](https://doi.org/10.1016/s0378-3758(01)00294-4). Looks just at an upper bound and constructs a uniformly most powerful test for $n \leq 2$. So maybe a good benchmark? (Theorem 5.2 and equation 5.3 seem to describe the test)

Shekhar & Ramdas 2023: [On the near-optimality of betting confidence sets for bounded means](https://arxiv.org/pdf/2310.01547). Another asymptotic bound on test efficiency, but with respect to a specific distribution. Claims Waudby-Smith & Ramdas 2023 is "near optimal" (weird, since we know it is weak) - compare just to an "Empirical Bernstein" bound. Looks at the asymptotic behaviour of $\limsup_{n\to \infty} \sqrt{n} w_n$ where $w_n$ is the width of the interval.

Maurer, A. and Pontil, M. Empirical Bernstein bounds and sample variance penalization. In Proceedings of the Twenty-Second Annual Conference on Learning Theory, pp. 115â€“124, 2009. Phan & Thomas & Learned-Miller 2021 claim this works poorly Paper has an inequality, but focuses on an application of the inequality. The inequality only uses mean and variance, hence likely weak.

Bentkus's concentration result - https://www.jstor.org/stable/3481693 (using only sum, so has to be weak, but better than Hoeffding) We won't discuss tests based on Hoeffding as it is shown to be substantially worse than many of the alternatives considered.

Bentkus & Zuijlen 2001: [Upper confidence bound for the mean](http://www.math.ru.nl/onderzoek/reports/rep2001/rep01_10.ps.gz) somewhat tightens other results, likely not so useful.

There are also more general frameworks looking at general moment inequalities e.g. [Romano, Shaikh & Wolf 2014](https://doi.org/10.3982/ECTA11011) or general functional inequalities e.g. [Lee, Song & Whang 2018](https://doi.org/10.1017/S0266466617000329), which we won't review further.

### Unprocessed refs I couldn't find

J. A. HORA and S. C. HORA. Nonparametric bounds for errors in dollar unit sampling based on the cumulative distribution function. Proceedings of the Decision Sciences Institute pp. 115â€”117 (1990). - Diouf & Dufour explain, claim far worse than asymptotic width (also symmetric around mean, which sounds bad).

V. K. SUTTON and D. M.YOUNG. A comparison of distribution-free confidence interval methods for estimating the mean of a random variable with bounded support. Baylor University, Department of Information Sys- tems (1997). -- Diouf & Dufour claim they find Fishman the best, Hora and Hora very wide.

