---
title: "Simulation-based calibration for Bayes Factors"
format: html
---

Throughout this paper we assume there is a single shared data space $Y$ and a set of Bayesian statistical models $\mathcal{M_i}$ for $i \in \{0, \dots, K-1\}$, each model associated and parameter space $\Theta_i$. For $y \in Y, \theta \in \Theta_i$ each model implies the following joint, marginal, and posterior densities:

$$
\begin{align*}
    \pi^i_\text{joint}(y, \theta) &= \pi^i_\text{obs}(y | \theta) \pi^i_\text{prior}(\theta)\\
    \pi^i_\text{marg}\left(y \right) &= \int_\Theta \mathrm{d} \theta \: \pi^i_{\text{obs}}(y | \theta) \pi^i_\text{prior}(\theta)\\
    \pi^i_\text{post}(\theta | y) &= \frac{\pi^i_\text{obs}(y | \theta) \pi^i_\text{prior}(\theta)}{\pi^i_\text{marg}\left(y \right)}. 
\end{align*}
$$ When there is no risk of confusion we may omit the model index. We will also use $\pi^i_\text{marg}, \pi^i_\text{prior}$ (without argument) to denote the distributions themselves.

We will use $\phi_\text{post}$ and $\phi_\text{marg}$ to denote candidate densities/distributions that are not guaranteed to be correct.

We primarily focus on the two-model situation, but the results generalize quite directly to multiple model comparisons.

Given $y \in Y$, the Bayes factor (BF) of model $i$ against model $j$ can be understood as both the ratio of marginal distributions of the data and the ratio of posterior probabilities of the models relative to the ratio of their prior probabilities:

$$
BF_{i,j} = \frac{ \pi^i_\text{marg}\left(y \right) }{\pi^j_\text{marg}\left(y \right)} = 
\frac{\text{Pr}(\mathcal{M_i} | y)}{\text{Pr}(\mathcal{M_j} | y)} \frac{\text{Pr}(\mathcal{M_j})}{\text{Pr}(\mathcal{M_i})}
$$

Despite some criticism (cite), this is a commonly used method to compare models and do hypothesis testing in the Bayesian paradigm.

An important practical concern is how to compute Bayes factors and/or the marginal densities of data as this typically involves difficult numerical integration.

## Theoretical background

### Bayesian model averaging

Bayes factors are closely related to Bayesian model averaging (BMA). In BMA, we assume a supermodel for the data with observations space $Y$ and parameter space $\Theta_\text{BMA} = \{ (0, \theta_0 ) | \theta_0 \in \Theta_0\} \cup \dots \cup  \{ (K-1, \theta_{K-1} ) | \theta_{K-1} \in \Theta_{K-1}\}$ of the form:

$$
\begin{align*}
i &\sim \text{Categorical}(\text{Pr}(\mathcal{M}_0), \dots, \text{Pr}(\mathcal{M}_{K - 1}) )\\
\theta &\sim \pi^i_\text{prior} \\
y &\sim \pi_\text{obs}^i(\theta)
\end{align*}
$$

The posterior distribution $\pi_\text{BMA}(i | y)$ of $i$ is then fully determined by the marginal distributions and vice versa:

$$
\text{Pr}(\mathcal{M}_i | y) = \pi_\text{BMA}(i | y) = \frac{\pi^i_\text{marg}(y)\text{Pr}(\mathcal{M}_i)}{\sum_{j=0}^{K - 1} \pi^j_\text{marg}(y)\text{Pr}(\mathcal{M}_j)}
$$

And finally the posterior distribution of any quantity of interest is obtained by weighing the predictions of the individual models, i.e. for any quantity $\Delta$:

$$
\pi_\text{BMA}(\Delta | y) = \sum_{i=0}^{K - 1} \pi^i_\text{post}(\Delta | y) \pi_\text{BMA}(i | y)
$$

This weighted prediction is then typically the output of BMA.

Due to the tight connection between Bayes factors, marginal data distributions and BMA, computing one immediately lets us derive the others. And so a check of correctness for either is also a check of correctness for the others.

### Bayes factors and prediction calibration

There is a straightforward way to checking the correctness of posterior model probabilities - whenever $\pi_\text{BMA}(i | y) = p$, the true model should be $\mathcal{M}_i$ in $p$ of the cases. I.e. that the predictions are calibrated.

A candidate density $\phi_\text{BMA}(i | y)$ passess the *prediction calibration check* if:

$$
\forall x,i: 0\leq x \leq 1, i \in {0, \dots, K - 1}:\\
\text{Pr}(\mathcal{M_i} | \phi_\text{BMA}(i | y)  = x) = x 
$$

expanding this

$$
\frac{
   \int_Y \text{d}y \: \pi^i_\text{marg}(y) \mathbb{I}\left[\phi_\text{BMA}(i | y)  = x \right]}{ 
   \sum_{j = 0}^{K-1} \Pr(\mathcal{M}_j) \int_Y \text{d}y \: \pi^j_\text{marg}(y) \mathbb{I}\left[\phi_\text{BMA}(i | y)  = x \right]}  = x
$$

TODO: double check the equation above.

This requires simulations (we do not know real-world ground truth and don't believe our models too much either).

Running $S$ simulations and for each $s \in {1, \dots S}$ we obtain $i_s$, $y_s \sim \pi^i_\text{marg}$ and compute $\phi_\text{BMA}(j | y_s)$ for $j \in \{0, \dots, \}$.

Especially for binary case ($K = 2$) there is a large number of available methods (TODO survey) - the $K > 2$ case can then be handled by looking at the binary calibration of the top (highest probability) prediction or by looking at calibration of the predictions for model $i$ vs. all others for all $i$.

Binary prediction calibration will however be satisfied even if $\phi_\text{BMA} \neq \pi_\text{BMA}$ --- most notably, the check is satisfied if we ignore the data $\phi(i | y) = \text{Pr}(\mathcal{M}_i)$ or do not use the data completely (see appendix).

### Bayesian model consistency via simulations

Another approach to checking the correctness of Bayes factors is checking the correctness of the implied BMA supermodel. Two classes of approaches to checking Bayesian computation in general are found in the literature.

#### Simulation-based calibration checking

SBC relies on the joint distribution of the prior and posterior samples

$$
\pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{prior}(\tilde\theta) \pi_\text{obs}(y | \tilde\theta) \pi_\text{post}(\theta | y)
$$

we then obtain that conditional on $y$, the $\tilde\theta$ and $\theta$ are exchangeable, i.e. that

$$
\forall y \in Y, \theta, \tilde\theta \in \Theta: \pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{SBC}(y, \tilde\theta, \theta)
$$

This cannot be checked directly, so we make two changes: 1) check on average over $Y$ and 2) use a test quantity $f: \Theta \times Y \to \mathbb{R} \cup \{-\infty,\infty\}$. For each simulation we then compare the ranks (sample SBC) or CDF (continuous SBC) of $f$ to discrete or continuous uniform distribution. Note that $f$ is used only for the ordering it implies. This is identical to checking that for all $0 < x < 1$, all posterior credible intervals of width $x$ contain the simulated value $x$ of the time.

In other words, we can take the prior as a single draw from the correct posterior.

When ties are possible, we need to randomly break them.

SBC for Bayes Factors is in fact the same as binary prediction calibration, i.e. considering the BMA supermodel, $\phi_\text{BMA}$ passes continous SBC w.r.t. $f((i, \theta), y) = i$ if and only if $\phi_\text{BMA}$ passess the prediction calibration check.

More generally for any $f: \Theta \times Y \to \{0,1\}$, $\phi$ passes continuous SBC w.r.t $f$ if and only if:

$$
\forall x: \text{Pr}\left(f(\theta, y) = 1 | x = \int_\Theta \: \text{d}\theta \: f(\theta, y)\phi(\theta | y)\right) = x 
$$

-   see Appendix for proof.

But when using SBC instead of just binary calibration, we can extend the check --- we can test the correctness of the full BMA model.

An important property of SBC is that by allowing test quantity $f$ to depend on $y$, we can in principle detect any mismatch between $\phi_\text{post}$ and $\pi_\text{post}$ --- we only need a suitable test quantity that is sensitive to the problem at hand. Modrák et al. provide theoretical and empirical evidence that the likelihood $f(\theta, y) = \pi_\text{obs}(y | \theta)$ is often highly sensitive in that regard.

#### Data-averaged posterior

The data-averaged posterior criterion checks that:

$$
\forall \theta: \pi_\text{prior}(\theta) = \int_Y \mathrm{d} y \: \phi_\text{post}(\theta |y)\int_\Theta \mathrm{d}\tilde\theta  \:  \pi_\text{obs}(y | \tilde\theta) \pi_\text{prior}(\tilde \theta)
$$

Typically not full joint distribution but univariate projections $f: \Theta \to \mathbb{R}$

Alternatively, we can compare the moments of the prior to the moments of the data-averaged posterior (cite).

The method of Schad et al. is to check the data-averaged posterior criterion for the model index in the BMA supermodel (i.e. for $f((i, \theta)) = i$).

A fundamental limitation is that data-averaged posterior ignores the mapping between data and posteriors --- it only checks that the observed Bayes factors look like reasonable Bayes factors for *some* model, but cannot check that they are correct for the model under investigation. For example, if we have uniform prior over the models and replace each Bayes factor with its inverse, although this violates prediction calibration and SBC. Data-averaged posterior also won't necessarily flag assigning posterior probability of 0 to the correct model, which is immediately flagged by SBC/prediction calibration.

Data-averaged posterior is repeatedly confused with SBC in the literature on model validation, but it is a fundamentally different check (e.g. because it is insensitive to some types of problems SBC is sensitive to as can be seen in the theoretical analysis above as well as practical examples below)

### Other approaches

The good check (cite). This shares some of the big problems with data-averaged posterior --- ignores the mapping between BFs and data. Additionally, both the variance and second raw moment of the Bayes factor in favor of the false hypothesis can have very large or even undefined variance, which explains the slow convergence. Even though large variance of BFs is not a problem for either SBC or data-averaged posterior (see appendix)

## Evaluation methods

The fact that a check can recognize an incorrectly computed Bayes factor/marginal likelihood/posterior model probability in principle does not mean it can recognize the problem in practice. We run simulation studies to show the power of various approaches to detect specific discrepancies.

We simulate $i$ and then $\theta_i$ and $y$ from the corresponding model. In all simulations we assume uniform prior over models. To investigate how the various test statistics evolve over time, we randomly sample 100 histories of the same length from the simulations (there are always at least 10 times more simulations than the length of a single history under investigation).

To check the data-averaged posterior following Schad et al. we use a one-sample t-test against the null of mean $\frac{1}{2}$ (alternatively: log posterior probability of $\mu = \frac{1}{2}$ from a Bayesian t-test with prior scale of the alternative matching the sd of uniform distribution over $[0,1]$).

For prediction calibration, we use the bootstrap-based miscalibration test based on [Dimitriadis, Gneiting, Jordan (2021)](https://doi.org/10.1073/pnas.2016191118).

For SBC we use the gamma statistic of Saailnoya (also used in Modrák et al.) - this cannot be easily turned into a p-value, so instead we report the log of the ratio of the statistic, to its 5th percentile under the null (i.e. lower than threshold => $p < 0.05$).

## Toy examples

### Single binary observation

First, let us start with a comparison of two extremely simple models with no parameters ($\theta_0 = \theta_1 = \emptyset$) problem:

$$
Y = \{0,1\}, \text{Pr}(y = 1 | \mathcal{M}_i) = a_i, a_0 < a_1\\
\text{Pr}(\mathcal{M}_0) = \text{Pr}(\mathcal{M}_1) = \frac{1}{2}
$$

Any candidate posterior can be described just by two numbers $\phi_\text{BMA}(i | Y = 1) = b_i$. Analytically, we can show that $\phi_\text{BMA}$ will satisfy the data-averaged posterior criterion if and only if $b_0 + b_1 = 1$, i.e. allowing infinitely many wrong Bayes factors, including assigning 100% probability to the wrong model or having $\phi_\text{BMA}(i | Y = 1) = \pi_\text{BMA}( 1 - i | Y = 1)$.

In contrast, $\phi_\text{BMA}$ will pass the prediction calibration check (and hence SBC for $f(i, y) = i$) only for the correct posterior and for the prior ($b_i = \text{Pr}(\mathcal{M}_i) = \frac{1}{2}$). The proof is that either $b_0 = b_1$ and then the model is calibrated only if $b_0 = b_1 = \frac{1}{2}$ or that $b_0 \neq b_1$ and then the model is calibrated only when $a_i = b_i$.

Additionally, checking SBC w.r.t. the likelihood $g(i, y) = a_i^y (1 - a_i)^{1-y}$ also rules out the $b_i = \frac{1}{2}$ option.

How quickly do we find problems empirically?

This model is however somewhat artificial - only two possible Bayes factor values, lets move to a slightly more realistic example.

### Poisson vs. negative binomial

$$
\begin{align*}
\mathcal{M}_0: y_1,\dots,y_N &\sim \text{Poisson}(\lambda)\\
\mathcal{M}_1: y_1,\dots,y_N &\sim \text{NB}_2(\lambda, \phi)\\
\text{Pr}(\mathcal{M}_0) &= \text{Pr}(\mathcal{M}_1) = \frac{1}{2}
\end{align*}
$$

With both variables known - in our experiments we used $\lambda = 3, \phi = 5$ and $N = 25$ as this provides for a wide range of observed BF values. In that setting, the correct BF is just the likelihood ratio of the two models.

-   DAP is slow to find problems
-   SBC allows detecting posterior = prior

### Summary of toy examples

In more general models, SBC with any realistic number and choice of test quantities may permit many wrong posteriors. However, those wrong posteriors are hard to construct and are unlikely to be the result of a bug in the computation. On the other hand, wrong posteriors satisfying the data-averaged posterior check are easy to come by and can plausibly arise due to a programming error (e.g. flipping the model indices) or bad numerical approximations.

## Real examples

### Schad et al.

Are BMA quantities useful?

### Something where bridgesampling works
