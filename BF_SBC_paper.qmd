---
title: "Simulation-based calibration checking for Bayes Factors"
author: Martin Modrák, Paul Bürkner
format: 
  pdf:
    fig-width: 7
    fig-height: 4
execute: 
  echo: FALSE
editor: 
  markdown: 
    wrap: 90
---

```{r setup}
#| include: FALSE
library(dplyr)
library(ggplot2)
library(patchwork)
library(SBC)
devtools::load_all()

cache_dir <- here::here("cache")

theme_set(theme_minimal())

format_p <- function(p) {
  case_when(p < 0.001 ~ "< 0.001",
            p < 0.01 ~ paste0("= ", round(p,3)),
            TRUE ~  paste0("= ", round(p,2))
            )
}

format_t_test_diff <- function(t_res) {
  paste0(round(t_res$conf.int - t_res$null.value, 3), collapse = "; ")
}
```

<!--
TODO: Absolutely necessary: 

- Real example: Bad vectorization/normalization (based on
paper) 
   - Turtles variant, but with normal prior on ranef stddev, "Forget" normalizing
constant 
- Ranef correct figures 

Important: 
- Schad bridgesampling 2x2 (claimed miscalib, actually OK) 
- Find a failure mode for bridgesampling/lmbf (turtles?!) 
- Gaffke history 
- Explain why SBC and prediction calibration get different results in finite samples (PDF vs CDF / histogram vs. CDF) 
- Explain what SBC does with the K \> 2 case (especially how ordering matters). Discuss possible
solutions.

Nice to have: 
- Practical example - close to zero probability for the correct model. (maybe weaker, "on a more theoretical note") 
- contrast with https://arxiv.org/pdf/2406.19346 
- Test & Discuss effect of prior on shared parameters in ranef model
- Miscalibration as not only p, but also CI (if interpretable)

=======
-->

## Introduction

Overall intro on Bayes factors/model selection... TODO

goals of the paper.


Despite some criticism (cite), this is a commonly used method to compare models and do
hypothesis testing in the Bayesian paradigm.

An important practical concern is how to compute Bayes factors and/or the marginal
densities of data as this typically involves difficult numerical integration.

## Theoretical background

Throughout this paper we assume there is a single shared data space $Y$ and a set of
Bayesian statistical models $\mathcal{M_i}$ for $i \in \{0, \dots, K-1\}$, each model
associated and parameter space $\Theta_i$. For $y \in Y, \theta \in \Theta_i$ each model
implies the following joint, marginal, and posterior densities:

$$
\begin{aligned}
    \pi^i_\text{joint}(y, \theta) &= \pi^i_\text{obs}(y | \theta) \pi^i_\text{prior}(\theta)\\
    \pi^i_\text{marg}\left(y \right) &= \int_\Theta \mathrm{d} \theta \: \pi^i_{\text{obs}}(y | \theta) \pi^i_\text{prior}(\theta)\\
    \pi^i_\text{post}(\theta | y) &= \frac{\pi^i_\text{obs}(y | \theta) \pi^i_\text{prior}(\theta)}{\pi^i_\text{marg}\left(y \right)}.
\end{aligned}
$$

When there is no risk of confusion we may omit the model index. We will also use
$\pi^i_\text{marg}, \pi^i_\text{prior}$ (without argument) to denote the distributions
themselves.

We will use $\phi_\text{post}$ and $\phi_\text{marg}$ to denote candidate
densities/distributions that are not guaranteed to be correct.

We primarily focus on the two-model situation, but the results generalize quite directly
to multiple model comparisons.

Given $y \in Y$, the Bayes factor (BF) of model $i$ against model $j$ can be understood as
both the ratio of marginal distributions of the data and the ratio of posterior
probabilities of the models relative to the ratio of their prior probabilities:

$$
BF_{i,j} = \frac{ \pi^i_\text{marg}\left(y \right) }{\pi^j_\text{marg}\left(y \right)} = 
\frac{\text{Pr}(\mathcal{M_i} | y)}{\text{Pr}(\mathcal{M_j} | y)} \frac{\text{Pr}(\mathcal{M_j})}{\text{Pr}(\mathcal{M_i})}
$$


### Bayesian model averaging

Bayes factors are closely related to Bayesian model averaging (BMA). In BMA, we assume a
supermodel for the data with observation space $Y$ and parameter space
$\Xi = \{ (0, \theta_0 ) | \theta_0 \in \Theta_0\} \cup \dots \cup  \{ (K-1, \theta_{K-1} ) | \theta_{K-1} \in \Theta_{K-1}\}$
of the form:

$$
\begin{aligned}
i &\sim \text{Categorical}(\text{Pr}(\mathcal{M}_0), \dots, \text{Pr}(\mathcal{M}_{K - 1}) )\\
\theta_i &\sim \pi^i_\text{prior} \\
y &\sim \pi_\text{obs}^i(\theta_i)
\end{aligned}
$$

The posterior distribution $\pi_\text{BMA}(i | y)$ of the model index $i$ is then fully determined by the
marginal distributions and vice versa:

$$
\text{Pr}(\mathcal{M}_i | y) = \pi_\text{BMA}(i | y) = \frac{\pi^i_\text{marg}(y)\text{Pr}(\mathcal{M}_i)}{\sum_{j=0}^{K - 1} \pi^j_\text{marg}(y)\text{Pr}(\mathcal{M}_j)}
$$

And finally the posterior distribution of any quantity of interest is obtained by weighing
the predictions of the individual models, i.e. for any quantity $\Delta$:

$$
\pi_\text{BMA}(\Delta | y) = \sum_{i=0}^{K - 1} \pi^i_\text{post}(\Delta | y) \pi_\text{BMA}(i | y)
$$

This weighted prediction is then typically the output of BMA. We further note that

$$
\frac{\pi_\text{BMA}(i | y)}{\pi_\text{BMA}(j | y)} = \frac{\text{Pr}(\mathcal{M}_i | y)}{\text{Pr}(\mathcal{M}_j | y)} = 
\frac{\text{Pr}(y| \mathcal{M}_i) \text{Pr}(\mathcal{M}_i)}{\text{Pr}(y | \mathcal{M}_j) \text{Pr}(\mathcal{M}_j)}   =
BF_{i,j}\frac{\text{Pr}(\mathcal{M}_i)}{\text{Pr}(\mathcal{M}_j)}
$$

Due to this tight connection between Bayes factors, marginal data distributions and BMA,
computing one immediately lets us derive the others. And so a check of correctness for
either is also a check of correctness for the others.

In the next sections we discuss some identities that need to hold for valid Bayes
factors/BMA posteriors and thus can be employed to check if a computation is correct.
Implicitly, all the theory below holds only for infinitely precise computation and average
over infinitely many simulations.

### Bayes factors and prediction calibration

There is a straightforward way to check the correctness of posterior model probabilities
with respect to the BMA supermodel - whenever $\pi_\text{BMA}(i | y) = p$, the true model
should be $\mathcal{M}_i$ in $p$ of the cases. I.e. that the predictions are calibrated.

Formally, given a candidate density $\phi_\text{BMA}(i | y)$, denote the
set of datasets that give model $k$ probability $p$ as
$\text{Pred}^\phi_{k, p} = \left\{y \in Y | \phi_\text{BMA}\left(k | y \right)  = p\right\}$.

A candidate 
passess the *prediction calibration check* if:

$$
\forall p,k: 0\leq p \leq 1,  k \in \{0, \dots, K - 1\}:\\
\sum_{i=0}^{K - 1}\int_{\text{Pred}^\phi_{k, p}} \text{d}y \  \pi^i_\text{marg}(y) > 0 \implies \text{Pr}(\mathcal{M}_k | y \in \text{Pred}^\phi_{k, p} ) = p 
$$

<!--
expanding this

$$
\frac{
   \text{Pr}(\mathcal{M}_k)\int_{\text{Pred}^\phi_{k, p}} \text{d}y \: \pi^k_\text{marg}(y) }{ 
   \sum_{j = 0}^{K-1} \Pr(\mathcal{M}_j) \int_{\text{Pred}^\phi_{k, p}} \text{d}y \: \pi^j_\text{marg}(y) }  = p
$$
-->

This requires simulations (we do not know real-world ground truth and don't believe our
models too much either).

Running $S$ simulations and for each $s \in {1, \dots S}$ we obtain $i_s$,
$y_s \sim \pi^{i_s}_\text{marg}$ and compute $\phi_\text{BMA}(j | y_s)$ for
$j \in \{0, \dots, K - 1\}$.

Especially for binary case ($K = 2$) there is a large number of available methods for
checking prediction calibration (TODO survey) - the $K > 2$ case can then be handled by
looking at the binary calibration of the top (highest probability) prediction or by
looking at calibration of the predictions for model $i$ vs. all others for all $i$.

Binary prediction calibration will however be satisfied even if
$\phi_\text{BMA} \neq \pi_\text{BMA}$ --- most notably, the check is satisfied if we
ignore the data $\phi(i | y) = \text{Pr}(\mathcal{M}_i)$ or do not use the data completely
(see appendix).

### Bayesian model consistency via simulations

Another approach to checking the correctness of Bayes factors is checking the correctness
of the implied BMA supermodel. Two classes of approaches to checking Bayesian computation
in general are found in the literature.

#### Simulation-based calibration checking

SBC relies on the joint distribution of the prior and posterior samples

$$
\pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{prior}(\tilde\theta) \pi_\text{obs}(y | \tilde\theta) \pi_\text{post}(\theta | y)
$$

we then obtain that conditional on $y$, the $\tilde\theta$ and $\theta$ are exchangeable,
i.e. that

$$
\forall y \in Y, \theta, \tilde\theta \in \Theta: \pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{SBC}(y, \tilde\theta, \theta)
$$

In other words, we can take the prior as a single draw from the correct posterior.

The exchangeability implied by the above, cannot be checked directly, so we make two
changes: 1) check on average over $Y$ and 2) use a test quantity
$f: \Theta \times Y \to \mathbb{R} \cup \{-\infty,\infty\}$. For each simulation we then
compare the ranks (sample SBC) or CDF (continuous SBC) of $f$ to discrete or continuous
uniform distribution. Note that $f$ is used only for the ordering it implies, so any
totally-ordered set will do as the target of a test quantity. This is identical to
checking that for all $0 < x < 1$, all posterior credible intervals of width $x$ contain
the simulated value $x$ of the time.

When ties are possible, we need to randomly break them. A more formal definition
from Modrák et al. 2024 is:

TODO

We will also say that $\phi$ passes SBC w.r.t data-independent test quantity
$f : \Theta \to \mathbb{R} \cup \{-\infty,\infty\}$ when it passes SBC w.r.t.
$f^\prime : \forall y \in Y: f^\prime(\theta, y) = f(\theta)$.

When $K = 2$, then SBC for Bayes Factors is in fact the same as binary prediction
calibration, i.e. considering the BMA supermodel, $\phi_\text{BMA}$ passes continuous SBC
w.r.t. $f: \Theta_\text{BMA} \to \{0, 1\}, f((i, \theta_i)) = i$ if and only if
$\phi_\text{BMA}$ passes the prediction calibration check.

More generally for any $f: \Theta \times Y \to \{0,1\}$, $\phi$ passes continuous SBC
w.r.t $f$ if and only if:

$$
\forall p: \text{Pr}\left(f(\theta, y) = 1 | \int_\Theta \: \text{d}\theta \: f(\theta, y)\phi(\theta | y) = p\right) = p
$$

See Appendix for proof.

Despite the two checks having the same properties in theory (which here means in the limit of both infinite simulations and --- for SBC --- infite posterior draws per simulation), the performance of their practical implementations can differ when a finite number of simulations is used.



When $K \geq 2$, we can still run SBC for the model index and discover problems. One relevant result from Modrák et al. is that in this case, changing the order of the submodels can result in different $\phi$ passing the SBC check. However, in practice examples when a miscalibration is detected only when we reorder a variable are hard to find.

<!-- Even more generally, SBC has to pass (and calibration has to hold) also for any subset of
of $Y$.-->


An important property of SBC is that by allowing test quantity $f$ to depend on $y$, we
can in principle detect any mismatch between $\phi_\text{post}$ and $\pi_\text{post}$ ---
we only need a suitable test quantity that is sensitive to the problem at hand. Modrák et
al. provide theoretical and empirical evidence that the likelihood
$f(\theta, y) = \pi_\text{obs}(y | \theta)$ is often highly sensitive in that regard.
Applying that to the BMA supermodel, SBC can in principle detect any problem in Bayes
factor computation. [Yao & Domke (2023)](https://proceedings.neurips.cc/paper_files/paper/2023/hash/7103cd82de95a7b30983fcf74ba499ac-Abstract-Conference.html) show that we can instead train a classifier that tries to distinguish prior and posterior draws and this allows sensitivity to any type of problem without explicitly constructing a test quantity. Unfortunately, this comes at a price of requiring a potentially very large number of simulations.

#### Data-averaged posterior

The data-averaged posterior criterion checks that:

$$
\forall \theta: \pi_\text{prior}(\theta) = \int_Y \mathrm{d} y \: \phi_\text{post}(\theta |y)\int_\Theta \mathrm{d}\tilde\theta  \:  \pi_\text{obs}(y | \tilde\theta) \pi_\text{prior}(\tilde \theta)
$$

Typically we do not test the full joint distribution but univariate projections
$f: \Theta \to \mathbb{R}$. 
Alternatively, we can compare the moments of the full prior to the moments of the full
data-averaged posterior (cite).

The method of Schad et al. is to check the data-averaged posterior criterion for the model
index in the BMA supermodel (i.e. with $f: \Theta_\text{BMA} \to \{0, \ldots K - 1\}$ and
$\forall (i, \theta_i) \in \Theta_\text{BMA}: f((i, \theta_i)) = i$), although they always
look at just two model comparisons, making the response actually binary.

Data-averaged posterior is repeatedly confused with SBC in the literature on model
validation, but it is a fundamentally different check. In full generality, data-averaged
posterior and SBC are not comparable --- there are test quantities and candidate
posteriors that will pass one but fail the other. However, whenever $\phi$ passes SBC for
a binary test quantity $f : \Theta \times Y \to \{0, 1\}$ (which is equivalent to having
calibrated predictions for that quantity) it will also have the correct data-averaged
posterior w.r.t. $f$. This follows, because correct calibration of a binary quantity
implies correct average of that quantity, full proof in the Appendix.

A fundamental limitation is that data-averaged posterior ignores the mapping between data
and posteriors --- it can at best ensure that there exists a set of models
$\mathbf{M^\prime} = \{\mathcal{M}^\prime_0, \ldots, \mathcal{M}^\prime_{K-1}\}$ over the
data space $Y$, such that
$\forall i \in \{0, \ldots,K-1\}, y \in Y: \phi_\text{BMA}(i|y) = \text{Pr}(\mathcal{M}^\prime_{K-1} |y)$
where the probability is with respect to $\mathbf{M^\prime}$. However, we can never rule
out that the models implied by $\phi_\text{BMA}$ are very different from what we expected.
Some examples of specific model mixups that can never be detected by data-averaged
posterior check include:

-   **All models identical**, i.e.
    $\mathcal{M}^\prime_0 = \ldots = \mathcal{M}^\prime_{K-1}$
-   **Flipped model indices**, i.e.
    $\forall i: \mathcal{M}^\prime_i = \mathcal{M}_{K - i - 1}$. When $K = 2$ this corresponds to computing
    the inverse of the correct Bayes factor.
-   **Different outcome family** in the context of regression models. E.g. assuming
    $\mathcal{M}_i$ are generalized linear models with gamma response,
    $\mathcal{M}^\prime_i$ could be normal linear regression models.
-   **Different predictor sets** in the context of regression models. E.g. assuming
    $\mathcal{M}_i$ are linear models, $\mathcal{M}^\prime_i$ could be linear regression
    models with arbitrary predictor sets present in $Y$.

Note that all of those problems are not implausible to arise from a simple programming
error in Bayes factor computation. The first two problems will violate both prediction
calibration and SBC (even without extra test quantities) in all cases. The latter two will
almost certainly fail those checks as well, although one may be able to construct some
contrived counterexamples. SBC with suitable test quantities will discover all problems.

Additional, but likely less relevant problem is that data-averaged posterior also won't
necessarily flag assigning posterior probability of 0 to the correct model, which is
immediately flagged by SBC/prediction calibration. In practice, posterior probability will
almost never be computed as exactly zero, but a package may repeatedly assign very low
probability to the correct model and still pass data-averaged posterior.


#### Posterior SBC

Good choice of priors is important for practical success with SBC.
There are at least three problems one can encounter in this regard:
1) Some statistical packages use improper priors, which we cannot simulate from.
2) Many priors are unrealistic --- notably using independent priors
for all parameters often results in large prior probability on unrealistic datasets
(TODO cite R2D2 + makemyprior). This both reduces relevance of our simulations
for real datasets and may result in convergence problems even when the model works
well enough for real datasets.
3) If we aim to verify Bayes factor computation for a specific dataset, 
even a very good prior will result in most simulated datasets being very unlike
the dataset we are interested in, making us less likely to notice potential problems
that manifest only in the particular region of data space.

All three problems can be overcome with *posterior SBC* (cite Aki's new preprint).

The overall idea of posterior SBC is to fit the model to a (usually small) dataset(s) and treat
the resulting posterior(s) as the prior for simulations. More formally, if 
we assume the data space partitions as $Y = Y_1 \times Y_2$, then given a fixed dataset $y_1 \in Y_1$
we can construct a new model $\bar\pi$ such that:

$$
\begin{aligned}
\bar\pi_\text{prior}(\theta) &=  \pi_\text{post}(\theta | y_1) \\
\bar\pi_\text{post}(\theta | y_2) &= \pi_\text{post}(\theta | (y_1, y_2)) \\
\end{aligned}
$$

We can then simulate from $\bar\pi_\text{prior}$ by fitting $\pi$ to $y_1$ and 
taking draws from $\pi_\text{post}(\theta|y_1)$. We may then use this parameter draw
to simulate $y_2$ from $\pi_\text{obs}$ and fit $\pi$ to the whole
dataset $(y_1,y_2)$. 
This setup can be generalized further by replacing the single fixed $y_1$ with a distribution $g$ over possible $y_1$, so we have $\bar\pi_\text{prior}(\theta) \int_{Y_1} \text{d}y_1 \: g(y_1)  \pi_\text{post}(\theta | y_1)$.

To check computation for models with improper priors, we need to choose $y_1$ such that $\pi_\text{post}(\theta | y_1)$ is proper and amenable to sampling. 
To provide a better prior, we take the distribution $g$ as representing prior beliefs about the data (which tend to be easier to elicit than
priors on parameters - cite Paul)
To target an SBC check
to a specific dataset, we may take a suitable portion of the dataset as fixed $y_1$ or even take $g$ to be a suitable
subsampling distribution of the dataset.

To use posterior SBC with Bayes factors, we need to compute the implied prior for 
the parameters of both models, but we also need the new prior for the model index $\bar\pi_\text{BMA}(i|y_1)$. 
Another important factor in choosing $y_1$ thus needs to be that it does not produce too extreme implied prior probability for any 
of the models under consideration.

Alternatively, we can tweak the
original prior probability, to get uniform distribution in the implied prior, i.e. $\bar\pi_\text{BMA}(i|y_1) = \frac{1}{K}$.


### Good check

The [Good check](https://link.springer.com/article/10.3758/s13428-024-02491-4) (cite). 
It notes that the expected Bayes factor in favor of the wrong hypothesis is 1, i.e.:


$$
\mathbb{E}(BF_{0,1} | \mathcal{M}_1) = 1
$$

which is a special case of a more general identity:

$$
\forall k \in \mathbb{N}:\mathbb{E}(BF^{k + 1}_{0,1} | \mathcal{M}_1) = \mathbb{E}(BF^k_{0,1} | \mathcal{M}_0)
$$

Sekulovski et al. recommend empirical testing of the identity for $k = 0$ and $k = 1$ via simulations.
We immediately see that the Good check shares some of the
big problems with data-averaged posterior --- it also completely ignores the mapping between BFs and data
and only checks the overall distribution of BFs.
Additionally, there are huge numerical difficulties. The distribution of BFs (and their powers) from simulations can have very large or even undefined variance, which makes the sample mean a poor estimator of the expected value.

For example, assume the following models with no parameters and a single observation $y \in \mathbb{R}$:

$$
\begin{split}
\mathcal{M}_0: y \sim \text{Cauchy}(0, 1) & \hspace{4em}
\mathcal{M}_1: y \sim N(0, 1) 
\end{split}
$$

In this case, $\mathbb{Var}(BF_{0,1} | \mathcal{M}_1)$ is undefined. This is admittedly a bit extreme, most models do not feature Cauchy marginal data distributions. So let's assume that for a fixed, known $\mu$ we instead have $\mathcal{M}_1: y \sim \text{N}(\mu, 1)$. Then we have $\mathbb{Var}(BF_{0,1} | \mathcal{M}_1) = \mathbb{Var}(BF_{1,0} | \mathcal{M}_0) = \exp\left(\mu^2\right) - 1$, so e.g. already for $\mu = 2$ we need over 5 000 simulations just to bring the standard error on the expectation below $\frac{1}{10}$. See appendix for full calculation.

The absence of any bound on variance of the estimate explains the lack of convergence for some cases 
and slow convergence for others as reported by Sekulovski et al. It also prevents
us from evaluating the remaining uncertainty we have about the expectations, as we cannot rule out very rare very large BF values.
We note that the same cases presented above pose no problem for both SBC and
data-averaged posterior (see appendix).

For those reasons, we will not evaluate the good check further in this paper.

<!-- TODO https://arxiv.org/pdf/2306.03580 -->

## Evaluation methods

The fact that a check can recognize an incorrectly computed Bayes factor/marginal
likelihood/posterior model probability in principle does not mean it can reliably
recognize the problem in practice. We run simulation studies to show the power of various
approaches to detect specific discrepancies.

We simulate $i$ and then $\theta_i$ and $y$ from the corresponding model. In all
simulations we assume uniform prior over models. To investigate how the various test
statistics evolve over time, we randomly sample 100 histories of the same length from the
simulations (there are always at least 10 times more simulations than the length of a
single history under investigation).

For Bayes factors, checking the data-averaged posterior requires a test for the
mean of bounded variable with unknown distribution. In most practical cases,
the posterior model probabilities are well-behaved enough for a classical t-test
to work well, but there are cases where it fails or provides inadequate coverage --- 
for example when all posterior probabilities are identical. 
Here we complement the t-test with the test of Gaffke 2005, which is more
thoroughly investigated in Learned-Miller and Thomas 2019 [A new confidence interval for
the mean of a bounded random variable](https://arxiv.org/pdf/1905.06208) --- this test has
been proven valid only in special circumstances, but no counterexample is known and it
provides much larger power than the alternatives.

<!-- To check the data-averaged posterior following Schad et al. we use a one-sample t-test -->
<!-- against the null of mean $\frac{1}{2}$ (alternatively: log posterior probability of -->
<!-- $\mu = \frac{1}{2}$ from a Bayesian t-test with prior scale of the alternative matching -->
<!-- the sd of uniform distribution over $[0,1]$, but this is computationally less stable and -->
<!-- makes data-averaged posterior look even worse). -->

For prediction calibration, we use the bootstrap-based miscalibration test based on
[Dimitriadis, Gneiting, Jordan (2021)](https://doi.org/10.1073/pnas.2016191118).

For SBC we use the gamma statistic of Saailnoya (also used in Modrák et al.) - this cannot
be easily turned into a p-value, so instead we report the log of the ratio of the
statistic, to its 5th percentile under the null (i.e. lower than threshold =\>
$p < 0.05$).

## Toy examples

### Single binary observation

First, let us start with a comparison of two extremely simple models with no parameters
($\theta_0 = \theta_1 = \emptyset$):

$$
Y = \{0,1\}, \: \text{Pr}(y = 1 | \mathcal{M}_0) = \frac{1}{3}, \: \text{Pr}(y = 1 | \mathcal{M}_1) = \frac{2}{3}\\
\text{Pr}(\mathcal{M}_0) = \text{Pr}(\mathcal{M}_1) = \frac{1}{2}
$$

Any candidate posterior can be described just by two numbers
$\phi_\text{BMA}(\mathcal{M}_1 | Y = y) = b_y$. Analytically, we can show that
$\phi_\text{BMA}$ will satisfy the data-averaged posterior criterion if and only if
$b_0 + b_1 = 1$, i.e. allowing infinitely many wrong Bayes factors, including assigning
100% probability to the wrong model or having
$\phi_\text{BMA}(\mathcal{M}_1 | Y = 0) = 1 - \pi_\text{BMA}( \mathcal{M}_1 | Y = 0)$. To
be specific the data-averaged posterior criterion for this model is:

$$
\forall i \in \{0,1\}: \text{Pr}(\mathcal{M}_i) = \sum_{y=0}^1  \phi_\text{BMA}(i | Y = y) \sum_{k=0}^1 \text{Pr}(y | \mathcal{M}_k) \text{Pr}(\mathcal{M}_k)
$$

We note that averaged over the models, both values of $y$ are equally likely, i.e.
$\sum_{k=0}^1 \text{Pr}(y | \mathcal{M}_k) \text{Pr}(\mathcal{M}_k) = \frac{1}{2}$

$$
\frac{1}{2} = \frac{1}{2} \sum_{y=0}^1  b_y \wedge
\frac{1}{2} = \frac{1}{2}\sum_{y=0}^1  (1 - b_y)
$$

which both reduce to $b_0 + b_1 = 1$.

In contrast, $\phi_\text{BMA}$ will pass the prediction calibration check (and hence SBC
for $f(i, y) = i$) only for the correct posterior and for the prior
($b_i = \text{Pr}(\mathcal{M}_i) = \frac{1}{2}$). The proof is that either $b_0 = b_1$ and
then the model is calibrated only if $b_0 = b_1 = \frac{1}{2}$ or that $b_0 \neq b_1$ and
then the model is calibrated only when $b_0 = \frac{1}{3}, b_1 = \frac{2}{3}$.

Additionally, checking SBC w.r.t. the likelihood $g(i, y) = a_i^y (1 - a_i)^{1-y}$ also
rules out the $b_i = \frac{1}{2}$ option.

How quickly do we find problems empirically? First let us assure ourselves that when the
model is correct we find no problems --- in the simulations, we use
$a_0 = 0.2, a_1 = 0.9$. See @fig-binary-correct.

```{r fig-binary-correct, message=FALSE}
#| fig-cap: Histories of check statistics for the correct binary model. The horizontal blue line marks rejecting the hypothesis of calibration with 5% false positive rate. We see that in all cases, violations tend to be non-severe and short-lived A) shows the log gamma statistic of default SBC, B) is the bootstrapped miscalabration of Dimitraidis et al. and C) is the t-test for the data-averaged posterior.
#| fig-height: 2.5
#| fig-width: 7

hist_binary_correct <- load_histories("binary_correct", "binary_example.qmd")

scale_x_correct <- scale_x_continuous("Number of simulations", breaks = c(0,500,1000))
(plot_log_gamma_histories(hist_binary_correct$log_gamma) + scale_x_correct | 
plot_log_p_histories(hist_binary_correct$miscalibration, "Miscalibration") + scale_x_correct | 
plot_log_p_histories(hist_binary_correct$schad, "DAP - Schad et al.") + scale_x_correct) + plot_annotation(tag_levels = "A") + plot_layout(widths = c(2,1,1), axis_titles = "collect_x")
```

Now what happens, if we compute the Bayes factors as if the probabilities were flipped
(i.e. computing the inverse of the true Bayes factor), see @fig-binary-flipped.

```{r fig-binary-flipped}
#| fig-cap: Histories of check statistics for the flipped binary model. The horizontal blue line marks rejecting the hypothesis of calibration with 5% false positive rate. The vertical orange line marks the number of simulations when the check first attains 80% power. We see that SBC as well as miscalibration identify the problem very quickly, while data-averaged posterior (DAP) does not diagnose it (the rare low p-values arise from using a t-test on what is essentially binary data). Note that the p-values from the miscalibration test are capped at $\frac{1}{2000}$ due to the number of bootstrap samples used.
#| fig-width: 7
#| fig-height: 2.5

hist_binary_flip <- load_histories("binary_flip", "binary_example.qmd")
(plot_log_gamma_histories(hist_binary_flip$log_gamma) | 
plot_log_p_histories(hist_binary_flip$miscalibration, "Miscalibration") | 
plot_log_p_histories(hist_binary_flip$schad, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A") + plot_layout(widths = c(2,1,1), axis_titles = "collect_x")
```

This model is however somewhat artificial - only two possible Bayes factor values, lets
move to a slightly more realistic example.

### Poisson vs. negative binomial

Here, we used the following model:

$$
\begin{aligned}
\mathcal{M}_0: y_1,\dots,y_N &\sim \text{Poisson}(\lambda)\\
\mathcal{M}_1: y_1,\dots,y_N &\sim \text{NB}_2(\lambda, \phi)\\
\text{Pr}(\mathcal{M}_0) &= \text{Pr}(\mathcal{M}_1) = \frac{1}{2}
\end{aligned}
$$

With both variables known - in our experiments we used $\lambda = 3, \phi = 5$ and
$N = 25$ as this makes the observed posterior probabilities of models values cover the
full $[0,1]$ interval. In that setting, the correct BF is just the likelihood ratio of the
two models.

Beyond the model index, we use two derived quantities in SBC, one is the log-likelihood,
second is an estimate of variance, specifically
$f(i, y) = \begin{cases}\text{Mean}(y) & i = 0 \\ \text{Var}(y) & i = 1 \end{cases}$ ---
this is an example of a simple data-dependent test quantity designed with the specific
models in mind: under model 0, mean should be equal to variance, so we use it in that
case. It is here to show, that one does not need to bring in the full model likelihood
(which might be laborious to re-implement) to gain benefits of derived quantities.

Let us start with the case that the the posterior probabilities are all $\frac{1}{2}$ ---
we know that neither data-averaged posterior, nor miscalibration can detect this in
principle (and that holds in the simulations), but data-dependent quantities typically
discover the problem reasonably quickly - see @fig-pnb-ignore-all.

```{r fig-pnb-ignore-all}
#| fig-cap: Histories of check statistics for poisson-NB model ignoring all data in BF computation. 

hist_pnb_ignore_all <- load_histories("pnb_ignore_all", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_ignore_all$log_gamma) / (
plot_log_p_histories(hist_pnb_ignore_all$miscalibration, "Miscalibration") +
  plot_spacer()
) +
#plot_log_p_histories(hist_pnb_ignore_all$schad, "DAP - Schad et al.")) + 
  plot_annotation(tag_levels = "A")
```

One may argue that this is easy to discover, simply check for constant BFs. So the next
step is a situation where just half of the datapoints are ignored - this still cannot be
detected by miscalibration or data-averaged posterior, but SBC will detect it with suitable derived
quantities. The posterior is now closer to correct, so we require more simulations to
discover the problem, but both data-dependent quantities uncover the issue, see
@fig-pnb-ignore-half.

```{r fig-pnb-ignore-half}
#| fig-cap: Histories of check statistics for poisson-NB model ignoring half the data in BF computation. 

hist_pnb_ignore_half <- load_histories("pnb_ignore_half", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_ignore_half$log_gamma) / (
plot_log_p_histories(hist_pnb_ignore_half$miscalibration, "Miscalibration") + 
plot_log_p_histories(hist_pnb_ignore_half$schad, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A")
```

Now let us try to introduce noise into the BF, specifically we will add normal noise with
sd 2 to the logarithm of the BF - relatively quickly discovered by miscalibration and SBC,
but generally missed by data-averaged posterior, see @fig-pnb-noise.

```{r fig-pnb-noise}
#| fig-cap: Histories of check statistics for poisson-NB model with noise in BF. 

hist_pnb_vari <- load_histories("pnb_vari", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_vari$log_gamma) / (
plot_log_p_histories(hist_pnb_vari$miscalibration, "Miscalibration") + 
plot_log_p_histories(hist_pnb_vari$schad, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A")
```

There is however a case where data-averaged posterior works better than the other options, that is when the
main problem is overall bias in the BF (e.g. because of wrong normalization constant in
one of the models) - here we add 2 to the logarithm of the Bayes factor for all
simulations. This is discovered relatively quickly by all of the options, but data-averaged posterior requires
the fewest simulations.

```{r}
#| fig-cap: Histories of check statistics for biased poisson-NB model. 

hist_pnb_bias <- load_histories("pnb_bias", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_bias$log_gamma) / (
plot_log_p_histories(hist_pnb_bias$miscalibration, "Miscalibration") + 
plot_log_p_histories(hist_pnb_bias$schad, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A")
```

### Summary of toy examples

In more general models, SBC with any realistic number and choice of test quantities may
permit many wrong posteriors. However, when using suitable data-dependent test quantities,
those wrong posteriors are hard to construct and are unlikely to be the result of a bug in
the computation. On the other hand, wrong posteriors satisfying the data-averaged
posterior check are easy to come by and can plausibly arise due to a programming error
(e.g. flipping the model indices, not including a part of the data) or bad numerical
approximations (increased variance, but no bias).

## Realistic examples

### Turtles example in the `bridgesampling` package

Interestingly enough, the first example in the documentation of the bridgesampling
package produces slightly miscalibrated Bayes factors.

The model for $H_1$ is a probit regression model with a single varying intercept, specifically:

$$
\begin{aligned}
y_i &\sim  \text{Bernoulli}(\Phi(\alpha + \beta x_{i} + \gamma_{g_i})),  &i = 1,2,\ldots,N\\
\gamma_j &\sim N(0, \tau),  &j = 1,2,\ldots, G \\
\alpha &\sim N(0, \sqrt{10}),\\
\beta &\sim N(0, \sqrt{10}),\\
\pi_\text{prior}(\tau^2) &= \left(1 + \tau^2\right)^{-2}.
\end{aligned}
$$

where $\Phi$ is the normal cumulative distribution function. The $H_0$ model is then the same but with the 
varying intercept removed, i.e. we set $\tau = 0$.


### Posterior SBC and the importance of using correct priors

When our models use improper priors for some parameters, it may be tempting to 
use a fixed value of $\mu$ and $\sigma^2$ or draw them from some
surrogate distribution. In this case however, the assumptions of all of the consistency
criteria (except the good check) do not hold. And indeed, we see violations of all
checks.

Problems are largest when a parameter with improper prior interacts with some parameter of
actual interest. A practically highly relevant example is the interaction of the prior on
the intercept (which is often taken as improper) with the prior on random effects. (see
Ogle, K., & Barber, J. J. (2020). Ensuring identifiability in hierarchical mixed effects
Bayesian models. Ecological Applications. doi:10.1002/eap.2159 for more discussion of this
problem).

We test a linear regression model with a single binary predictor and a single ranodm effect. 
We will be using the BayesFactor package, which implies the following parametrization and prior distributions of the model:

$$
\begin{aligned}
y_i &\sim  N(\alpha + \beta x_{i} + \gamma_{g_i}, \sigma),  &i = 1,2,\ldots,N\\
\gamma_j &\sim N\left(0, \frac{\sqrt{2}}{4}\sigma \sqrt{\tau} \right), &j = 1,2,\ldots, G \\
\tau &\sim \text{InvGamma}\left(\frac{1}{2},\frac{1}{2} \right),\\
\beta &\sim N\left(0, \frac{\sqrt{2}}{4} \sigma \right),\\
\pi_\text{prior}(\alpha_0, \sigma^2) &= \frac{1}{\sigma^2}.
\end{aligned}
$$

Where $x_i = \pm \frac{\sqrt{2}}{2}$ is a binary predictor coded following the standard BayesFactor encoding. Note that the implied prior on $\gamma_j$ is multivariate Cauchy and the prior on $\alpha, \sigma^2$ is improper. 
Without modifications, we therefore cannot sample from the prior on $\alpha, \sigma^2$.

```{r}
ranef_test <- load_precomputed_file(here::here("cache/lmbf_ranef_post_presence_100_tests.rds"), "lmbf_ranef_presence_post_sbc.qmd")
ranef_ci_diff <- format_t_test_diff(ranef_test$t)

res_ranef_post <- load_precomputed_file(here::here("cache/lmbf_ranef_post_presence_100.rds"), "lmbf_ranef_presence_post_sbc.qmd")$result
n_ranef_sims <- length(res_ranef_post)

```

If we choose the overall intercept and residual standard deviation as fixed constants 
(here $\alpha = 0, \sigma = 1$) we obtain an apparent miscalibration
for both the model index and all the individual random effect parameters (see
@fig-ranef-constant). In this case the problem is the most quickly picked up by the
miscalibration test with data-averaged posterior and SBC for the model index following very quickly. However
this is a false positive. Employing posterior SBC using $y_1$ as a dataset with 4 groups and 3 observations each (to make all
parameters well identified for sampling) produces a proper prior for the mean
and intercept and results in perfect calibration in all methods even after running `r format(n_ranef_sims, big.mark = " ")`
simulations (@fig-ranef-post, miscalibration p `r format_p(ranef_test$miscalibration_p)`,
95% CI for data-averaged posterior difference from expected mean `r ranef_ci_diff`). So we can conclude that
the BF calculation in the BayesFactor package in fact works well for this case.

```{r fig-ranef-constant}
#| fig-cap: Histories of check statistics for random effect model using constant mean and intercept. 

hist_ranef_constant <- load_histories("ranef_constant", "lmbf_ranef_presence_post_sbc.qmd")
plot_log_gamma_histories(hist_ranef_constant$log_gamma) / (
plot_log_p_histories(hist_ranef_constant$miscalibration, "Miscalibration") + 
plot_log_p_histories(hist_ranef_constant$schad, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A")
```

```{r fig-ranef-post}
#| fig-cap: Result of calibration checks for random effect model using posterior SBC. A) empirical CDF difference plots for model parameters (g_Subject --- standard deviation of the random intercepts, model --- model index, mu --- overall intercept, sig2 --- residual variance, Subject --- merged plot for all random intercepts, x1 --- fixed effect) B) calibration plot.
ecdf_ranef_post <- plot_ecdf_diff(res_ranef_post, combine_variables = combine_lmBF_arrays)
bp_post <- binary_probabilities_from_stats(res_ranef_post$stats) 
#SBC::plot_binary_calibration(bp_post)
reliabilitydiag_post <- my_reliability_diag(bp_post)

((ecdf_ranef_post + theme(legend.position = "bottom")) | reliabilitydiag_post) + plot_layout(widths = c(2,1)) + plot_annotation(tag_levels = "A")
```

TODO: here we might want to also compare BFs calculated assuming standard improper prior
vs. some weakly informative prior on intercept + sigma for the same set of datasets. I'd
expect it will turn out to have noticeable effect despite a pretty common recommendation
that priors on shared parameters don't matter that much. But maybe that's too tangential
for this paper?

The previous example is the simplest setting we discovered where we obtain incorrect
strong miscalibration due to improper priors for shared parameters, however, the problem
subtly affects even much simpler settings.

```{r}
ttest_fixed <- load_precomputed_file(here::here("cache/ttest_fixed_stats.rds"), "ttestBF.qmd")
```


One example is the `ttestBF` in BayesFactor package, representing the JZS test, we focus on the one-sample variant, specifically the $\mathcal{M}_1$ model is:

$$
\begin{aligned}
y_{1}, ..., y_n &\sim N(\delta\sigma, \sigma) \\
\delta &\sim \text{Cauchy}\left(0, \frac{\sqrt{2}}{2}\right) \\
\pi_\text{prior}\left(\sigma^2\right) &= \frac{1}{\sigma^2}
\end{aligned}
$$
and the null model $\mathcal{M}_0$ assumes $\delta = 0$.
Note the improper Jeffrey's prior on variance. We will use $n = 5$ in our simulations to provide a reasonably wide distribution of observed BFs.
When we fix $\sigma = 1$, the default statistics and calibration look good. 
However, when we test SBC for a derived quantity that combines the model index ($i$)
with the observed sd of the data, specifically $f(i, y) = (i - \frac{1}{2})(\text{sd}(y) - 1)$
we see a strong apparent miscalibration (@fig-ttest-fixed A). 
Another way to see that is that for
datasets where the observed sd is high, the posterior probabilities in favor of the 
alternative hypothesis tend to be too low, which is matched by a corresponding overestimation
when the observed sd is low (for datasets with  $\text{sd}(y) \geq 1$ miscalibration p `r format_p(ttest_fixed$miscalibration_p_high)`,   @fig-ttest-fixed B,C). Similar, but more subtle 
miscalibration is also observed when we put a proper prior on $\sigma$ ---
we tested $\pi_\text{prior}\left(\sigma^2\right) = \frac{1}{1 + \sigma^2}$ and
$\sigma \sim \text{HalfCauchy}(0, 1)$, see Appendix for detailed results.

```{r fig-ttest-fixed}
#| fig-cap: Results of SBC when using a fixed value of $\sigma$ when simulating datasets for Bayesian t-test. A) Histories of log gamma statistics for the model index, posterior mean (mu) and $f(i, y) = (i - \frac{1}{2})(\text{sd}(y) - 1)$ (model_sd_m1). We see that only the latter diagnoses any problems. B) Calibration plot across all simulations shows no problems. C) and D) Separate calibration plots for datasets with high/low standard deviation show problems.
plot_log_gamma_histories(ttest_fixed$hist |> mutate(variable = factor(variable, levels = c("model", "mu", "model_sd_m1")))) /
(ttest_fixed$reliability_diag + ggtitle("All simulations") | ttest_fixed$reliability_diag_high + ggtitle("sd(y) >= 1") | ttest_fixed$reliability_diag_low  + ggtitle("sd(y) < 1")) + plot_annotation(tag_levels = "A")
```



```{r}
ttest_post <- load_precomputed_file(here::here("cache/ttest_post_stats.rds"), "ttestBF.qmd")
ttest_ci_diff <-  format_t_test_diff(ttest_post$t)

n_test_sim <- ttest_post$t$parameter["df"] + 1
```


When we use posterior SBC (with $y_1 = (-1,1)$), the computed Bayes factors pass all checks to high
precision --- we have run `r format(n_test_sim, big.mark  = " ")` simulations and obtained miscalibration p `r format_p(ttest_post$miscalibration_p)`,
95% CI for data-averaged posterior difference from expected mean `r ttest_ci_diff`) and no problems with SBC including all
derived quantities. So we conclude that the `ttest` function in fact works well and
the above miscalibrations are simply a result of not matching the simulations to
the model.

### Schad & Vasishth 2024

[Schad & Vasishth (2024)](https://arxiv.org/pdf/2406.08022) apply their
previously published method to show that in some settings Bayes factors from
popular packages are biased. In particular, they claim there is bias in "2x2 repeated 
measures design with crossed random effects for subjects and items" using both the BayesFactors
as well as bridgesampling with brms fits. Their method is using the data-averaged posterior check 
(which they incorrectly label as SBC)

Unfortunately, their work has multiple problems making their conclusions invalid.
The authors only shared their code for the bridgsampling case, but it is plausible
the same problems appear in the results for the BayesFactors package as the reported 
miscalibration is very similar in both cases.

Most importantly Schad & Vasishth simulate from either a full model or a model where three coefficients 
are zero, but then compute BF for three different hypotheses, each testing whether a single coefficient is zero.
Additionally, they treat several variance parameters of random effects as fixed in simulation
but treat them as unknown parameters in the model and there is a bug in their use of the LKJ prior 
for random effect correlations. 

```{r}
schad <- load_precomputed_file(here::here("cache/schad_fixed_stats.rds"), "schad_biased_bf_fixed.qmd")
n_schad_sim <- schad$t$parameter["df"] + 1
schad_ci_diff <- format_t_test_diff(schad$t)
```


Using their simulation code for the bridgesampling case, fixing the bugs
and matching the model to the simulation code we obtain calibrated Bayes factors. We have run `r format(n_schad_sim, big.mark  = " ")` simulations and obtained miscalibration p `r format_p(schad$miscalibration_p)`, 95% CI for data-averaged posterior difference from expected mean `r schad_ci_diff`.

Similar problems affect also the situations where they claim the Bayes factors are calibrated.
Further
they handle the improper priors
used in the methods from BayesFactors package by taking a fixed value for the parameter, which --- as we have shown ---
can result in invalid checks. When we reran simulations with a more careful checks we however did not
uncover any additional problems.

Another minor problem is that they use Bayesian t-tests for testing that the 
data-averaged posterior is equal to the prior mean, which can be invalid
in the same settings as a classical t-test, but can additionally have computational
issues even for cases when classical t-test would perform well.

### Discovering bad normalization constants

Neglecting vectorization https://link.springer.com/article/10.1007/s41237-024-00232-7 
The authors note that 9 out of 21 investigated Stan models neglected the normalizing constant for
truncated distribution, leading to possibly biased Bayes factors.
 - reuse the turtle models
 
TODO

## Discussion

While there are cases where data-averaged posterior performs somewhat better than SBC/prediction calibration,
it also fails catastrophically in many examples where SBC and prediction calibration work. 
Using data-averaged posterior for validation of Bayes factor
computation should thus be considered only as a supplement to SBC or prediction
calibration. The recently proposed good check is in our view very rarely useful.

In the examples we ran, prediction calibration checks tended to have slightly higher power to detect discrepancies than SBC for the model index and sometimes even than SBC for the log likelihood. The reason is throwing away information via random tiebreaking --- increasing power of SBC in those settings is an ongoing research project of ours and we consider it out of scope of this paper.
This speaks in favor of using prediction calibration for testing Bayes factor computations. However, there are issues that prediction calibration missed that were only visible when using data-dependent test quantities in SBC. So we argue that prediction calibration is useful as an _addition_ to SBC, not as a replacement.

Another advantage of running SBC for the full BMA model is that we can use the same
simulations to check that the individual models are well implemented/calibrated --- if the
individual models are wrong, we cannot make any strong inferences about the method for
computing Bayes factors.

Our results also show that to rule out problems in Bayes factor computation, simulation studies need
to use a lot of simulations --- even quite strong miscalibration can require several
hundred simulations to be reliably uncovered (e.g. @fig-pnb-ignore-half).


Using multiple test quantities or even multiple methods for checking the calibration of
the model yields to multiple testing issues. However, here Type II error (false negatives)
is the biggest problem. We can usually run more simulations to discerns flukes from real
problems with little expense. Additionally, in a perfect world a failed calibration check
is followed by investigation to get understanding why the computation failed.

A practical problem we encountered with checking some of the off-the-shelf Bayes factors methods
was that the priors in use are at best partially documented. In contrast, when `brms` or
a Stan program was used to compute the models the priors are transparently accessible.

In our tests, both `bridgesampling` and the BayesFactors package provided mostly reliable
Bayes factor estimates. Bias previously reported by Schad & Vasishth was an artifact
of invalid testing procedure. The only problem we have seen was in the
`bridgesampling` package and testing the presence of random effect in TODO interpretation.

We have also shown that the choice of priors for parameters shared across both compared
models like the intercept or standard deviation (which is often given little thought) can
have non-negligible effect on Bayes factors, further highlighting the brittleness of Bayes
factors.

Recalibration with SBC does not work in general http://www.stat.columbia.edu/~gelman/research/unpublished/calibration_toy.pdf

## Appendix

### Proofs and theory

We will denote the integral of $f(x)$ w.r.t.\ $x$ over domain $X$ as $\int_X \mathrm{d}x \: f(x)$. $\mathbb{I}[P]$ is the indicator function for a given predicate $P$. When a function can be understood as describing a conditional probability distribution, we will use $|$ to separate the function arguments we condition on. This is only to assist comprehension and has the same semantic meaning as using a comma.

In all cases, we assume an underlying statistical model $\pi$ which decomposes into a prior and observational model. Given a data space $Y$ and a parameter space $\Theta$, then for $y \in Y, \theta \in \Theta$ the model implies the following joint, marginal and posterior distributions:
%
\begin{gather*}
    \pi_\text{joint}(y, \theta) = \pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{marg}\left(y \right) = \int_\Theta \mathrm{d} \theta \: \pi_{\text{obs}}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{post}(\theta | y) = \frac{\pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)}{\pi_\text{marg}\left(y \right)}.
\end{gather*}
%
Unless noted otherwise, all definitions and proofs implicitly assume a single model $\pi$ is given.

\begin{definition}[Posterior family] Given a data space $Y$ and a parameter space $\Theta$, a \emph{posterior family} $\phi$ assigns a normalized posterior density to each possible $y \in Y$. I.e. posterior family is a function $\phi : \Theta \times Y  \rightarrow  \mathbb{R^{+}}$ such that
%
\begin{equation*}
\forall y: \int \mathrm{d}\theta \:\phi(\theta | y) = 1.
\end{equation*}
%
For each $y$, we will denote the implied distribution over $\Theta$ as $\phi_y$. 

\end{definition}



\begin{definition}[Test quantity]
     Given a data space $Y$ and a parameter space $\Theta$ a \emph{test quantity} is any measurable function $f :\Theta \times Y  \to  \mathbb{R} $.
\end{definition}

In all of the below Given a model $\pi$, data space $Y$, a parameter space $\Theta$, a posterior family $\phi$, and a test quantity $f$, we define


**Definition:** A candidate posterior $\phi$ satisfies binary calibration check w.r.t. 
$f: \Theta \times Y \to \{0,1\}$ if

$$
\forall p: 0\leq p \leq 1:\\
 \text{Pr}\left(f(\theta, y) = 1 | \int_\Theta \: \text{d}\theta \: f(\theta, y)\phi(\theta | y) = p \right) = p 
$$

wherever the left-hand side is well-defined (i.e. when there are $y \in Y$ such that $\int_\Theta \: \text{d}\theta \: f(\theta, y)\phi(\theta | y) = p$)

We note that for BMA supermodel with two submodels the definition is equivalent to the prediction calibration for the model index.


#### Binary calibration, SBC and data-averaged posterior

We will work with continuous SBC. 
Continuous SBC is implies sample SBC and is more relevant, since for BFs we obtain (an approximation of) the
posterior CDF, not just samples.


**Definition:** (Continuous rank CDF, continuous $q$, continuous SBC)
We first define fitted CDF: $C_{\phi,f}: \bar{\mathbb{R}}\times Y\to [0,1]$, $C_{\phi,f}(s | y) := \int_{\Theta}\mathrm{d}\theta\,\mathbb{I}\left[f\left(\theta, y \right) \leq s\right]\phi\left(\theta| y\right)$ and fitted tie probability: $D_{\phi,f}: \bar{\mathbb{R}}\times Y\to [0,1]$, 
    $D_{\phi,f}(s | y) := \int_\Theta \mathrm{d}\theta \: \phi(\theta | y) \mathbb{I}\left[ f(\theta, y) = s \right]$

We then define the \emph{continuous } $q: [0,1] \times Y \to [0,1]$ as
\begin{equation*}
    q_{\phi,f}(x|y) := 
\int_\Theta \mathrm{d} \tilde\theta \: \pi_\text{post}(\tilde\theta | y) \mbox{Pr}\left(C_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) - U D_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) \leq x\right),
\end{equation*}
%
assuming $U$ is a random variable distributed uniformly over the $[0, 1]$ interval. 



Finally, \emph{$\phi$ passes continuous SBC w.r.t.\ $f$} if $
\forall x \in [0, 1]: \int_Y \mathrm{d}y \: q_{\phi,f}(x|y) \pi_\text{marg}(y)  = x$.
\end{definition}



**Theorem:** For any $f: \Theta \times Y \to \{0,1\}$, $\phi$ passes continuous SBC
w.r.t $f$ if and only if it satisfies binary calibration check w.r.t $f$.


**Proof:** First let us explore how $q_{\phi,f}(x|y)$ looks when we restrict the image of $f$ to $\{0,1\}$. Directly from definitions we get:

$$
C_{\phi, f}(s | y) = \begin{cases}
   0 & s < 0 \\
   D_{\phi, f}(0 | y) & 0 \leq s < 1 \\
   1 & 1 \leq s
\end{cases} 
\\
D_{\phi, f}(0 | y) = 1 - D_{\phi, f}(1 | y)
$$

$$
\mbox{Pr}\left(C_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) - U D_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) \leq x \right) \\
=\begin{cases}
\mbox{Pr}\left((1 - U)D_{\phi, f}(0 | y) \leq x\right) & f(\tilde\theta, y) = 0 \\
\mbox{Pr}\left(1 - UD_{\phi, f}(1 | y) \leq x\right) & f(\tilde\theta, y) = 1 \\
\end{cases} \\
=\begin{cases}
F_u(x |0, D_{\phi, f}(0 | y)) & f(\tilde\theta, y) = 0 \\
F_u(x |D_{\phi, f}(0 | y), 1) & f(\tilde\theta, y) = 1 \\

\end{cases}\\


$$

where $F_u(x | a, b)$ is the CDF of the continuous uniform distribution on the interval $[a,b]$ evaluated at $x$.
We will denote 

$$
F_0(x | y) = F_u(x |0, D_{\phi, f}(0 | y)) \\
F_1(x | y) = F_u(x |D_{\phi, f}(0 | y), 1) 
$$

$$
q_{\phi,f}(x | y) = \sum_{i = 0}^1 \int_{\tilde\theta:f(\tilde\theta, y) = i} \mathrm{d} \tilde\theta \: \pi_\text{post}(\tilde\theta | y) F_i(x | y) = \sum_{i = 0}^1 D_{\pi, f}(i | y) F_i(x | y)
$$

For $i \in \{0,1\}$ we denote:

$$
F_i(x) = \int_Y \mathrm{d} y \: \pi_\text{marg}(y)D_{\pi, f}(i | y) F_i(x | y) \\
= \int_Y \mathrm{d} y \int_{\tilde\theta:f(\tilde\theta, y) = i} \mathrm{d} \tilde\theta \: \pi_\text{post}(\tilde\theta | y) F_i(x | y)
$$

Note that $F_i(x)$ can be interpreted as the probability of obtaining expected $f$ of at most $x$ in a simulation.

And the SBC condition for $f$ becomes:

$$
F_0(x) + F_1(x) = x
$$

$$
F_i(x) = \mbox{Pr}(f(\theta, y) \leq x | f(\tilde\theta,y) = i)
$$



Assume we have a binary variable $M \in \{1,2 \}$ and $R \in [0,1]$ is the posterior probability that $M = 1$ averaging over data

The $q$ function given $M, R$ is then:

$$
q(x | M, R) = \begin{cases}
\frac{x}{R} & M = 1, x < R \\
1 & M = 1, x \geq R \\
0 & M = 2, x < R \\
\frac{x - R}{1 - R} & M = 2, x\geq R
\end{cases}
$$

Now let us take the prior probability $\mbox{Pr}(M = 1) = 1 - \mbox{Pr}(M = 2) = m$

Treating $q(x)$ as a random variable, we obtain for $0 < s < 1$:

$$
\mbox{Pr}(q(x) \leq s) = \mbox{Pr}\left(M = 1 \wedge x \leq R \wedge \frac{x}{r} \leq s \right) + \mbox{Pr}\left(M = 2 \wedge x \leq R \right) + \mbox{Pr}\left(M = 2 \wedge x \geq R \wedge \frac{x-R}{1-R} \leq s \right) = \\ = \mbox{Pr}\left(M = 1 \wedge x \leq Rs \right) + \mbox{Pr}\left(M = 2 \wedge \frac{x-R}{1-R} \leq s \right) = \\
= \mbox{Pr}\left(M = 1 \wedge R \geq \frac{x}{s} \right) + \mbox{Pr}\left(M = 2 \wedge R \geq \frac{x-s}{1-s} \right) 
$$

We now define the CDFs $F^R_i(x) = \mbox{Pr}(R \leq x | M = i)$

TODO: below holds only fo continuous distributions (for continuous P(X >= y) != 1 - F(y))

$$
\mbox{Pr}(q(x) \leq s) 
= \mbox{Pr}\left(M = 1 \wedge R \geq \frac{x}{s} \right) + \mbox{Pr}\left(M = 2 \wedge R \geq \frac{x-s}{1-s} \right) =\\
m \left(1 - F^R_1 \left(\frac{x}{s}\right) \right) + (1 - m) \left(1 - F^R_2\left(\frac{x-s}{1-s}\right) +   \right) = \\
1 - m F^R_1 \left(\frac{x}{s}\right)    - (1 - m) F^R_2\left(\frac{x-s}{1-s}\right)  \\
$$


Side check:

$$
\frac{x-R}{1-R} \leq s \\
x-R \leq s (1-R) \\
x - s \leq R(1 -s)\\
\frac{x - s}{1-s} \leq R\\
$$

For a positive rv $X$ we have

$$
E(X) = \int_0^\infty (1 - F^X(t)) \text{d}t
$$
We now define the cCDFs $C^R_i(x) = \mbox{Pr}(R \geq x | M = i)$

$$
\mbox{Pr}(q(x) \leq s) 
= \mbox{Pr}\left(M = 1 \wedge R \geq \frac{x}{s} \right) + \mbox{Pr}\left(M = 2 \wedge R \geq \frac{x-s}{1-s} \right) =\\
m C^R_1 \left(\frac{x}{s}\right) + (1 - m) C^R_2\left(\frac{x-s}{1-s}\right)
$$

$$
E(q(x)) = \int_0^1 1 - m C^R_1 \left(\frac{x}{s}\right)  - (1 - m) C^R_2\left(\frac{x-s}{1-s}\right) \text{d}s = \\
1 - m\int_0^1 C^R_1 \left(\frac{x}{s}\right) \text{d}s  - (1 - m) \int_0^1 C^R_2\left(\frac{x-s}{1-s}\right) \text{d}s = \\
1 - m\int_x^1 C^R_1 \left(\frac{x}{s}\right) \text{d}s  - (1 - m)(1-x) - (1 - m) \int_0^x C^R_2\left(\frac{x-s}{1-s}\right) \text{d}s = \\
m + x -mx - m\int_x^1 C^R_1 \left(\frac{x}{s}\right) \text{d}s  - (1 - m) \int_0^x C^R_2\left(\frac{x-s}{1-s}\right) \text{d}s = \\
$$

Lebesgue measure > 0???

Binary calibration means that for each subset of $Y$ with a given $p$ the $q$ function is diagonal, so it is diagonal on average over whole $Y$ -> SBC.

If continous SBC then no point masses can be miscalibrated. If it was, there has to be a ball near the "kinks" where SBC doesn't hold where there is no point mass (TODO, maybe incorrect)


This equivalence implies that all limitations of SBC directly apply to binary calibration. Notably,
following Theorem 7 of Modrák et al., if $f$ ignores data (e.g. when we do binary calibration for the model index), it will be satisfied when $\phi = \pi_{\text{prior}}.


Proof binary calibration OK -> DAP OK.

### Good check convergence

Assuming $\mathcal{M}_0: y \sim \text{Cauchy}(0, 1), \mathcal{M}_1: y \sim N(0, 1)$ we have

$$
BF_{0,1}(y) = \frac{\text{Cauchy}(y | 0, 1)}{\text{Norm}(y | 0,1)} = \sqrt{\frac{2}{\pi}} \frac{e^\frac{y^2}{2}}{1 + y^2} 
$$

and the second raw moment

$$
\mathbb{E}(BF_{0,1}^2 | \mathcal{M}_1) = \int_{-\infty}^\infty \frac{\left(\text{Cauchy}(y | 0, 1)\right)^2}{\text{Norm(y | 0,1)}} \text{d}y
$$

which is infinite because 

$$
\lim_{y \to \pm\infty} \frac{\left(\text{Cauchy}(y | 0, 1)\right)^2}{\text{Norm(y | 0,1)}} = \lim_{y \to \pm\infty} \frac{\sqrt{2} \exp\left(\frac{y^2}{2}\right)}{\pi^\frac{3}{2}(1 + y^2)^2} = \infty 
$$

and hence also the variance is infinite.

Now, let's assume $\mathcal{M}_0: y \sim N(\mu, 1)$ for a fixed and known $\mu$. Then the second raw moment is:

$$
\mathbb{E}(BF_{0,1}^2 | \mathcal{M}_1) = \int_{-\infty}^\infty \frac{\left(\text{Norm}(y | \mu, 1)\right)^2}{\text{Norm(y | 0,1)}} \text{d}y = \int_{-\infty}^\infty \frac{\exp\left(\frac{x^2}{2} - (x - \mu)^2\right)}{\sqrt{2 \pi}} \text{d}y \\
= \exp\left(\mu^2\right) \int_{-\infty}^\infty \frac{\exp\left(-\frac{1}{2} (x - 2\mu)^2\right)}{\sqrt{2 \pi}} \text{d}y = \exp\left(\mu^2\right)
$$

where in the last step, the integral is over the PDF of a normal distribution with mean $2\mu$ and standard deviation $1$. Therefore the variance of the Bayes factor is 

$$
\text{Var}(BF_{0,1}|\mathcal{M}_1) = \mathbb{E}(BF_{0,1}^2|\mathcal{M}_1) - E(BF_{0,1}|\mathcal{M}_1)^2 = \exp\left(\mu^2\right)  - 1.
$$ 

We note that this situation is completely symmetric in the sense that $\mathbb{E}(BF_{0,1}^2 | \mathcal{M}_1) = \mathbb{E}(BF_{1,0}^2 | \mathcal{M}_0)$ so the problems are not specific to one "direction" of the Good check.

Now assume that $\mu = 2$, then the standard error of the mean after $i$ iterations is $\sqrt{\frac{\exp(4) - 1}{n}}$ so we expect to need 5 360 simulations to keep the standard error belowe $\frac{1}{10}$ and 21 440 simulations to keep the standard error below $\frac{1}{20}$ and thus obtain estimates likely within $\frac{1}{10}$ of the correct value. 

Empirically, we can see the lack of any convergence for the Cauchy case and very slow convergence for the normal case in @fig-good-convergence. It follows, that the Good check cannot reliably diagnose BF computation unless we know that the BF distribution is well behaved. 

```{r fig-good-convergence}
#| fig-cap: Convergence of the good check when a single datapoint is simulated from the standard normal and  A) $\mathcal{M}_0: y \sim \text{Cauchy}(0, 1)$ --- here the variance is infinite and we see no convergence at all or B) $\mathcal{M}_0: y \sim N(2, 1)$ where the average Bayes factor eventually converges, but 50 000 simulations are not enough for reliable convergence. Each line is the cumulative average from a single set of simulations. The highlighted area shows average BF 0.9 - 1.1. Means from first 1000 simulations are not shown.
#| fig-width: 7
#| fig-height: 2.5
plot_good_example <- function(n_sims, n_runs, m0, step = 10, start = 1, linealpha = 0.3) {
   y <- matrix(rnorm(n_sims * n_runs), nrow = n_sims)
   if(m0 == "cauchy") {
     ll0 <- dcauchy(y, log = TRUE)
   } else {
     ll0 <- dnorm(y, mean = m0, log = TRUE)
   }
   ll1 <- dnorm(y, log = TRUE)
   bf <- exp(ll0 - ll1)
   cmeans <- apply(bf, MARGIN = 2, FUN = \(x) cumsum(x) / (1:length(x)))
   colnames(cmeans) <- 1:n_runs
   sims <- seq(start, n_sims, by = step)
   cmeans <- cmeans[sims,]
   plot_df <- as.data.frame(cmeans) |> mutate(sim = sims) |> 
     tidyr::pivot_longer(c(everything(), - all_of("sim")), names_to = "run", values_to = "mean_bf")
   plot_df |> ggplot(aes(x = sim, y = mean_bf, group = run)) + 
     geom_rect(xmin = start, xmax = n_sims, ymin = 0.9, ymax = 1.1, fill = "lightblue") +
     geom_hline(yintercept = 1, color = "darkblue") +
     geom_line(alpha = linealpha) +
     scale_y_continuous("Cumulative mean BF") +
     #scale_y_log10("Cumulative mean BF") +
     scale_x_continuous("Simulations") +
     expand_limits(y = c(0.9,1.1))
}

{
set.seed(68523321)
good_start <- 1000
good_step <- 10
good_linealpha <- 0.2
good_sims <- 50000
good_runs <- 20
(plot_good_example(good_sims, good_runs, m0 = "cauchy", step = good_step, start = good_start, linealpha = good_linealpha) | plot_good_example(good_sims, good_runs, m0 = 2, step = good_step, start = good_start, linealpha = good_linealpha)) + plot_layout(axis_titles = "collect") + plot_annotation(tag_levels = "A")
  
}

```


#### SBC for the same models


```{r fig-good-sbc}
#| fig-cap: ECDF difference plots from SBC (A) and calibration diagrams (B, C) for Bayes factors when $\mathcal{M}_1: y \sim N(0, 1)$ and either $\mathcal{M}_0: y \sim \text{Cauchy}(0, 1)$ or $\mathcal{M}_0: y \sim N(2, 1)$.
#| fig-width: 7
#| fig-height: 4
res_good_cauchy <- load_precomputed_file(here::here("cache/good_cauchy.rds"), "good_check_examples.qmd")$result
bp_good_cauchy <- binary_probabilities_from_stats(res_good_cauchy$stats)
t_good_cauchy <- format_t_test_diff(t.test(bp_good_cauchy$prob, mu = 0.5))

res_good_norm2 <- load_precomputed_file(here::here("cache/good_norm2.rds"), "good_check_examples.qmd")$result
bp_good_norm2 <- binary_probabilities_from_stats(res_good_norm2$stats)
t_good_norm2 <- format_t_test_diff(t.test(bp_good_norm2$prob, mu = 0.5))

stats_good_combined <- rbind(
  res_good_cauchy$stats |> mutate(variable = "Model - Cauchy"),
  res_good_norm2$stats |> mutate(variable = "Model - Normal"))

rel_good_cauchy <- my_reliability_diag(bp_good_cauchy)
rel_good_norm2 <- my_reliability_diag(bp_good_norm2)

tag_loc <- theme(plot.tag.location = "panel", plot.tag.position = c(-0.05,1.1))

((plot_ecdf_diff(stats_good_combined) + facet_wrap(~variable, ncol = 1) + labs(tag = "A") + theme(legend.position = "bottom") + tag_loc)  | (rel_good_cauchy + labs(tag = "B - Cauchy") + tag_loc) | (rel_good_norm2 + labs(tag = "C - Normal") + tag_loc)) + plot_layout(axis_titles = "collect_x")

n_sims_good <- nrow(bp_good_norm2)
stopifnot(n_sims_good == nrow(bp_good_cauchy))
```



On the contrary, SBC as well as prediction calibration and data-averaged posterior have no problem and since the BF is computed correctly, show that both scenarios pass the tests (to the precision available with the `r format(n_sims_good, big.mark = " ")` we ran). For SBC and prediction calibration see @fig-good-sbc. For data-averaged posterior, the 95% CI for the difference between prior and average posterior probability is `r t_good_cauchy` for the Cauchy scenario and `r t_good_norm2` for the Normal scenario.  


### Miscalibration with surrogate distributions for variance in `ttestBF`
