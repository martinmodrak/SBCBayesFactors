---
title: "Simulation-based calibration checking for Bayes Factors"
author: Martin Modr√°k, Paul B√ºrkner
pdf-engine: pdflatex
format: 
  pdf:
    fig-width: 7
    fig-height: 4
    keep-tex: true
    
execute: 
  echo: FALSE
  cache: true
fig-width: 7
fig-height: 4
editor: 
  markdown: 
    wrap: 90
knitr:
  opts_chunk: 
    dev: cairo_pdf    
---

This document reproduces all figures and numbers for the relevant sections.

```{r setup}
#| include: FALSE
#| cache: FALSE
library(dplyr)
library(ggplot2)
library(patchwork)
library(SBC)
devtools::load_all()

cache_dir <- here::here("cache")

theme_set(theme_minimal())

format_p <- function(p) {
  case_when(p < 0.001 ~ "< 0.001",
            p < 0.01 ~ paste0("= ", round(p,3)),
            TRUE ~  paste0("= ", round(p,2))
            )
}

format_t_test_diff <- function(t_res) {
  paste0(round(t_res$conf.int - t_res$null.value, 3), collapse = "; ")
}
```


## Toy examples

### Single binary observation

The various test statistics have the
expected random behavior and only rarely drop below the relevant thresholds (@fig-binary-correct).

```{r fig-binary-correct, message=FALSE}
#| fig-cap: Histories of check statistics for the correct binary model. The horizontal blue line marks rejecting the hypothesis of calibration with 5% false positive rate. We see that in all cases, violations tend to be non-severe and short-lived A) shows the log gamma statistic of default SBC, B) is the p-value of the bootstrapped miscalabration test of Dimitraidis et al. and C) is the p-value from t-test for the data-averaged posterior.
#| fig-height: 2.5
#| fig-width: 7

hist_binary_correct <- load_histories("binary_correct", "binary_example.qmd")

tag_theme <- theme(plot.tag.location = "plot", plot.tag.position = c(0, 1.01), plot.tag = element_text(hjust = 0, size = 12))

scale_x_correct <- scale_x_continuous("Number of simulations", breaks = c(0,500,1000))

(plot_log_gamma_histories(hist_binary_correct$log_gamma) + scale_x_correct + labs(tag = "A) SBC") + tag_theme | 
plot_log_p_histories(hist_binary_correct$miscalibration) + scale_x_correct + labs(tag = "B) Calibration") + tag_theme | 
plot_log_p_histories(hist_binary_correct$ttest) + scale_x_correct + labs(tag = "C) DAP") + tag_theme)   + plot_layout(widths = c(2,1,1), axis_titles = "collect_x")
```



```{r fig-binary-flipped}
#| fig-cap: Histories of check statistics for the flipped binary model. The horizontal blue line marks rejecting the hypothesis of calibration with 5% false positive rate. The vertical orange line marks the number of simulations when the check first attains 80% power. We see that SBC as well as miscalibration identify the problem very quickly, while data-averaged posterior (DAP) does not diagnose it (the rare low p-values arise from using a t-test on what is essentially binary data). Note that the p-values from the miscalibration test are capped at $\frac{1}{2000}$ due to the number of bootstrap samples used.
#| fig-width: 7
#| fig-height: 2.5

hist_binary_flip <- load_histories("binary_flip", "binary_example.qmd")
((plot_log_gamma_histories(hist_binary_flip$log_gamma) + labs(tag = "A) SBC") + tag_theme) | 
(plot_log_p_histories(hist_binary_flip$miscalibration) + labs(tag = "B) Calibration") + tag_theme) | 
(plot_log_p_histories(hist_binary_flip$ttest)) + labs(tag = "C) DAP") + tag_theme) + plot_layout(widths = c(2,1,1), axis_titles = "collect_x")
```


### Poisson vs. negative binomial



```{r fig-pnb-ignore-all}
#| fig-cap: "Histories of SBC gamma statistic for poisson-NB model ignoring all data in BF computation, the model index and two data-dependent derived quantities: the log likelihood (log_lik) and variance estimate (var_y)."
#| fig-width: 7
#| fig-height: 2.5

hist_pnb_ignore_all <- load_histories("pnb_ignore_all", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_ignore_all$log_gamma) #/ (
#plot_log_p_histories(hist_pnb_ignore_all$miscalibration, "Calibration") 
#+
#  plot_spacer()
#) +
#plot_log_p_histories(hist_pnb_ignore_all$ttest, "DAP - Schad et al.")) + 
#  plot_annotation(tag_levels = "A")
```


```{r fig-pnb-ignore-half}
#| fig-cap: Histories of SBC gamma statistics for poisson-NB model ignoring half the data in BF computation. 
#| fig-width: 7
#| fig-height: 2.5


hist_pnb_ignore_half <- load_histories("pnb_ignore_half", "poisson_nb_example.qmd")
plot_log_gamma_histories(hist_pnb_ignore_half$log_gamma) 
#/ (
#plot_log_p_histories(hist_pnb_ignore_half$miscalibration, "Calibration") + 
#plot_log_p_histories(hist_pnb_ignore_half$ttest, "DAP - Schad et al.")) + plot_annotation(tag_levels = "A")
```

```{r fig-pnb-noise}
#| fig-cap: Histories of check statistics for poisson-NB model with noise in BF.  A) shows the log gamma statistic of default SBC, B) is the p-value of the bootstrapped miscalabration test of Dimitraidis et al. and C) is the p-value from t-test for the data-averaged posterior.

hist_pnb_vari <- load_histories("pnb_vari", "poisson_nb_example.qmd")
(plot_log_gamma_histories(hist_pnb_vari$log_gamma) + labs(tag = "A) SBC") + tag_theme+ theme(axis.title.y = element_text(size = 9))) / (
(plot_log_p_histories(hist_pnb_vari$miscalibration) + labs(tag = "B) Calibration") + tag_theme) + 
(plot_log_p_histories(hist_pnb_vari$ttest)  + labs(tag = "C) DAP") + tag_theme))
```


```{r fig-pnb-bias}
#| fig-cap: Histories of check statistics for biased poisson-NB model.  A) shows the log gamma statistic of default SBC, B) is the p-value of the bootstrapped miscalabration test of Dimitraidis et al. and C) is the p-value from t-test for the data-averaged posterior.

hist_pnb_bias <- load_histories("pnb_bias", "poisson_nb_example.qmd")
(plot_log_gamma_histories(hist_pnb_bias$log_gamma) + labs(tag = "A) SBC") + tag_theme + theme(axis.title.y = element_text(size = 9))) / (
(plot_log_p_histories(hist_pnb_bias$miscalibration) + labs(tag = "B) Calibration") + tag_theme) + 
(plot_log_p_histories(hist_pnb_bias$ttest)  + labs(tag = "C) DAP") + tag_theme))
```


## Realistic examples


```{r}
turtles_metrics <- load_precomputed_file(here::here("cache/turtles_metrics.rds"), "turtles.qmd")
turtles_ci_diff <- format_t_test_diff(turtles_metrics$t)
implied_prior_rejection <- turtles_metrics$t$estimate[2]

turtles_post_half_metrics <- load_precomputed_file(here::here("cache/turtles_post_half_metrics.rds"), "turtles.qmd")
turtles_post_half_ci_diff <- format_t_test_diff(turtles_post_half_metrics$t)
```


In this example, rejections change the implied prior to $P(\mathcal{M}_1) \simeq `r round(implied_prior_rejection, 2)`$ and after running `r format(turtles_metrics$n_sims, big.mark = " ")` we find no problems `r report_success_metrics(turtles_metrics)`.


For the second check, ... but to show an alternative, we can also choose $P(M_1) \simeq `r round(turtles_post_half_metrics$prob1_tweaked_prior_half,2)`$,
which results in $P(M_1\mid y_1) = \frac{1}{2}$ 

After running `r format(turtles_post_half_metrics$n_sims, big.mark = " ")` simulations we see no calibration issues `r report_success_metrics(turtles_post_half_metrics)`.

### Discovering bad normalization constants


```{r fig-turtles-bad-norm}
#| fig-cap: Histories of check statistics for bad normalization constant in the turtles model.  A) shows the log gamma statistic of default SBC, B) is the p-value of the bootstrapped miscalabration test of Dimitraidis et al. and C) is the p-value from t-test for the data-averaged posterior.

hist_turtles_norm_bad <- load_histories("turtles_norm_bad", "turtles.qmd")
(plot_log_gamma_histories(hist_turtles_norm_bad$log_gamma) + labs(tag = "A) SBC") + tag_theme + theme(axis.title.y = element_text(size = 9))) / (
(plot_log_p_histories(hist_turtles_norm_bad$miscalibration) + labs(tag = "B) Calibration") + tag_theme) + 
(plot_log_p_histories(hist_turtles_norm_bad$ttest)  + labs(tag = "C) DAP") + tag_theme))
```

### Posterior SBC and the importance of using correct priors


```{r}
ranef_test <- load_precomputed_file(here::here("cache/lmbf_ranef_post_presence_100_metrics.rds"), "lmbf_ranef_presence_post_sbc.qmd")

res_ranef_post <- load_precomputed_file(here::here("cache/lmbf_ranef_post_presence_100.rds"), "lmbf_ranef_presence_post_sbc.qmd")$result
n_ranef_sims <- length(res_ranef_post)

```

 Employing posterior SBC using a dataset with 4 groups and 3 observations each as $y_1$ (to make all
parameters well identified for sampling) produces a proper prior for the intercept and standard deviations and results in perfect calibration in all methods even after running `r format(n_ranef_sims, big.mark = " ")`
simulations (@fig-ranef-post; `r report_success_metrics(ranef_test)`). 

```{r fig-ranef-constant}
#| fig-cap: Histories of check statistics for random effect model using constant intercept and standard deviation. 

hist_ranef_constant <- load_histories("ranef_constant", "lmbf_ranef_presence_post_sbc.qmd")
(plot_log_gamma_histories(hist_ranef_constant$log_gamma)  + labs(tag = "A) SBC") + tag_theme+ theme(axis.title.y = element_text(size = 9)))/ (
(plot_log_p_histories(hist_ranef_constant$miscalibration)  + labs(tag = "B) Calibration") + tag_theme) + 
(plot_log_p_histories(hist_ranef_constant$ttest)  + labs(tag = "C) DAP") + tag_theme))
#TODO: uncertainty about red line?
```

```{r fig-ranef-post}
#| fig-cap: Result of calibration checks for random effect model using posterior SBC. A) empirical CDF difference plots for model parameters (g_Subject --- standard deviation of the random intercepts, model --- model index, mu --- overall intercept, sig2 --- residual variance, Subject --- merged plot for all random intercepts, x1 --- fixed effect) B) calibration plot.

subject_transl <- paste0("ùõæ-",1:15)
names(subject_transl) <- paste0("Subject-",1:15)
ranef_post_name_transl <- c("g_Subject" = "ùúé‚àöùúè", "model" = "model", "mu" = "ùõº", "sig2" = "ùúé", subject_transl, "x1-A" = "ùõΩ")
res_ranef_post_transl <- res_ranef_post$stats |> filter(!grepl("^\\.", variable)) |> mutate(variable = ranef_post_name_transl[variable])
ecdf_ranef_post <- plot_ecdf_diff(res_ranef_post_transl, combine_variables = combine_lmBF_arrays) + scale_x_continuous(breaks = c(0, 0.5, 1))

metrics_ranef_post <- load_precomputed_file(here::here("cache/lmbf_ranef_post_presence_100_metrics.rds"), "lmbf_ranef_presence_post_sbc.qmd")
reliabilitydiag_post <- metrics_ranef_post$reliability_diag

((ecdf_ranef_post + theme(legend.position = "bottom")  + labs(tag = "A) SBC") + tag_theme) | (reliabilitydiag_post  + labs(tag = "B) Calibration") + tag_theme)) + plot_layout(widths = c(2,1)) 
```


```{r}
ttest_fixed <- load_precomputed_file(here::here("cache/ttest_fixed_metrics.rds"), "ttestBF.qmd")
```



(for datasets with  $\text{sd}(y) \geq 1$ miscalibration p `r format_p(ttest_fixed$high$miscalibration_stats$p)`,   @fig-ttest-fixed B,C). 

```{r fig-ttest-fixed}
#| fig-cap: Results of SBC when using a fixed value of $\sigma$ when simulating datasets for Bayesian t-test. A) Histories of log gamma statistics for the model index, posterior mean (mu) and $f(i, y) = (i - \frac{1}{2})(\text{sd}(y) - 1)$ (model_sd_m1). We see that only the latter diagnoses any problems. B) Calibration plot across all simulations shows no problems. C) and D) Separate calibration plots for datasets with high/low standard deviation show problems.
#| fig-width: 7
#| fig-height: 4


tag_theme_calib <- tag_theme + theme(plot.tag.position = c(0, 1.1), plot.margin = margin(l = 10, r = 10))

(plot_log_gamma_histories(ttest_fixed$hist |> mutate(variable = factor(variable, levels = c("model", "mu", "model_sd_m1")))) + labs(tag = "A) SBC") + tag_theme + theme(plot.margin = margin(10, 10, 20, 10))+ theme(axis.title.y = element_text(size = 9))) /
((ttest_fixed$all$reliability_diag  + labs(tag = "B) Calibration - all") + tag_theme_calib) | (ttest_fixed$high$reliability_diag  + labs(tag = "C) Calibration - sd(y) ‚â• 1") + tag_theme_calib) | 
   (ttest_fixed$low$reliability_diag  + labs(tag = "D) Calibration - sd(y) < 1") + tag_theme_calib))
```



```{r}
ttest_post <- load_precomputed_file(here::here("cache/ttest_post_metrics.rds"), "ttestBF.qmd")


```

When we use posterior SBC (with $y_1 = (-1,1)$), the computed Bayes factors pass all checks to high
precision --- we have run `r format(ttest_post$n_sim, big.mark  = " ")` simulations and see no sign of problems including all
derived quantities  (`r report_success_metrics(ttest_post, dap_digits = 4)`). 


### Results table 

```{r results ='asis'}
all_metrics <- list(
  "bridgesampling + Stan" = list(
    "Turtles" = turtles_metrics,
    "Turtles - post SBC" = turtles_post_half_metrics
  ),
  "BayesFactor" = 
    list("Random effect presence via lmBF" = ranef_test,
         "Bayesian t-test via ttestBF" = ttest_post
         )
  )

all_metrics |> purrr::imap_dfr(\(cases, package) {
  purrr::map_dfr(cases, success_metrics_for_table) |> 
    mutate(Package = c(package, rep("", times = length(cases) - 1)), Scenario = names(cases), .before = 1)
}) |> knitr::kable(format = "latex", booktabs = TRUE, caption = "My cap") |> print()


```


## Appendix

### Metrics to assess data-averaged posterior

```{r}
dap_correct <- load_precomputed_file(here::here("cache/dap_metrics_correct_summary.rds"), "dap_metrics.qmd")
dap_wrong <- load_precomputed_file(here::here("cache/dap_metrics_wrong_summary.rds"), "dap_metrics.qmd")

```

```{r fig-dap-probs}
p_correct <- dap_correct$probs_df |>  ggplot(aes(x = prob)) + geom_histogram(binwidth = 0.05) + facet_wrap(~scenario, scales = "free_y") + scale_x_continuous("Posterior model probability")
p_wrong <- dap_wrong$probs_df |>  ggplot(aes(x = prob)) + geom_histogram(binwidth = 0.05) + facet_wrap(~scenario, scales = "free_y")  + scale_x_continuous("Posterior model probability")

p_correct / p_wrong + plot_annotation(tag_levels = "A")
```

```{r tab-dap-correct}
dap_correct$summary |> dap_power_table()
```

```{r}
dap_wrong$summary |> dap_power_table()
```


```{r}
max_uncertainty <- function(summary) {
  max(summary$power - summary$ci_low, summary$ci_high - summary$power)
}
max_uncertainty(dap_correct$summary)
max_uncertainty(dap_wrong$summary)

```


### Good check convergence


Empirically, we can see the lack of any convergence for the Cauchy case and very slow convergence for the normal case in @fig-good-convergence. It follows, that the Good check cannot reliably diagnose BF computation unless we know that the BF distribution is well behaved. 

```{r fig-good-convergence}
#| fig-cap: | 
#|   Convergence of the good check when a single datapoint is simulated from the standard normal and  A) $\mathcal{M}_0: y \sim \text{Cauchy}(0, 1)$ --- here the variance is infinite and we see no convergence at all or B) $\mathcal{M}_0: y \sim N(2, 1)$ where the average Bayes factor eventually converges, but 50 000 simulations are not enough for reliable convergence. Each line is the cumulative average from a single set of simulations. The highlighted area shows average BF 0.9 - 1.1. Means from first 1000 simulations are not shown.
#| fig-width: 7
#| fig-height: 2.5
plot_good_example <- function(n_sims, n_runs, m0, step = 10, start = 1, linealpha = 0.3) {
   y <- matrix(rnorm(n_sims * n_runs), nrow = n_sims)
   if(m0 == "cauchy") {
     ll0 <- dcauchy(y, log = TRUE)
   } else {
     ll0 <- dnorm(y, mean = m0, log = TRUE)
   }
   ll1 <- dnorm(y, log = TRUE)
   bf <- exp(ll0 - ll1)
   cmeans <- apply(bf, MARGIN = 2, FUN = \(x) cumsum(x) / (1:length(x)))
   colnames(cmeans) <- 1:n_runs
   sims <- seq(start, n_sims, by = step)
   cmeans <- cmeans[sims,]
   plot_df <- as.data.frame(cmeans) |> mutate(sim = sims) |> 
     tidyr::pivot_longer(c(everything(), - all_of("sim")), names_to = "run", values_to = "mean_bf")
   plot_df |> ggplot(aes(x = sim, y = mean_bf, group = run)) + 
     geom_rect(xmin = start, xmax = n_sims, ymin = 0.9, ymax = 1.1, fill = "lightblue") +
     geom_hline(yintercept = 1, color = "darkblue") +
     geom_line(alpha = linealpha) +
     scale_y_continuous("Cumulative mean BF") +
     #scale_y_log10("Cumulative mean BF") +
     scale_x_continuous("Simulations") +
     expand_limits(y = c(0.9,1.1))
}

{
set.seed(68523321)
good_start <- 1000
good_step <- 10
good_linealpha <- 0.2
good_sims <- 50000
good_runs <- 20
(plot_good_example(good_sims, good_runs, m0 = "cauchy", step = good_step, start = good_start, linealpha = good_linealpha) | plot_good_example(good_sims, good_runs, m0 = 2, step = good_step, start = good_start, linealpha = good_linealpha)) + plot_layout(axis_titles = "collect") + plot_annotation(tag_levels = "A")
  
}

```


#### SBC for the same models


```{r fig-good-sbc}
#| fig-cap: |
#|   ECDF difference plots from SBC (A) and calibration diagrams (B, C) for Bayes factors when $\mathcal{M}_1: y \sim N(0, 1)$ and either $\mathcal{M}_0: y \sim \text{Cauchy}(0, 1)$ or $\mathcal{M}_0: y \sim N(2, 1)$.
#| fig-width: 7
#| fig-height: 4
res_good_cauchy <- load_precomputed_file(here::here("cache/good_cauchy.rds"), "good_check_examples.qmd")$result
bp_good_cauchy <- binary_probabilities_from_stats(res_good_cauchy$stats)
t_good_cauchy <- format_t_test_diff(t.test(bp_good_cauchy$prob, mu = 0.5))

res_good_norm2 <- load_precomputed_file(here::here("cache/good_norm2.rds"), "good_check_examples.qmd")$result
bp_good_norm2 <- binary_probabilities_from_stats(res_good_norm2$stats)
t_good_norm2 <- format_t_test_diff(t.test(bp_good_norm2$prob, mu = 0.5))

stats_good_combined <- rbind(
  res_good_cauchy$stats |> mutate(variable = "Model - Cauchy"),
  res_good_norm2$stats |> mutate(variable = "Model - Normal"))

rel_good_cauchy <- my_reliability_diag(bp_good_cauchy)
rel_good_norm2 <- my_reliability_diag(bp_good_norm2)

tag_loc <- theme(plot.tag.location = "panel", plot.tag.position = c(-0.05,1.1))

((plot_ecdf_diff(stats_good_combined) + facet_wrap(~variable, ncol = 1) + labs(tag = "A") + theme(legend.position = "bottom") + tag_loc)  | (rel_good_cauchy + labs(tag = "B - Cauchy") + tag_loc) | (rel_good_norm2 + labs(tag = "C - Normal") + tag_loc)) + plot_layout(axis_titles = "collect_x")

n_sims_good <- nrow(bp_good_norm2)
stopifnot(n_sims_good == nrow(bp_good_cauchy))
```

```{r}
good_cauchy_metrics <- load_precomputed_file(here::here("cache/good_cauchy_metrics.rds"), "good_check_examples.qmd")
cat("N sims Cauchy: ", good_cauchy_metrics$n_sims,"\n")
report_success_metrics(good_cauchy_metrics,dap_digits = 4)
```

```{r}
good_norm2_metrics <- load_precomputed_file(here::here("cache/good_norm2_metrics.rds"), "good_check_examples.qmd")
cat("N sims norm2: ", good_norm2_metrics$n_sims, "\n")
report_success_metrics(good_norm2_metrics,dap_digits = 4)
```



### Miscalibration with surrogate distributions for variance in `ttestBF`

```{r}
ttest_inv1psquared <- load_precomputed_file("cache/ttest_post_metrics_inv1psquared.rds", "ttestBF.qmd")
ttest_cauchy <- load_precomputed_file("cache/ttest_post_metrics_cauchy.rds", "ttestBF.qmd")
```


First we instead use a proper prior with similar form $\pi_\text{prior}(\sigma^2) = \frac{1}{1 + \sigma^2}$. We require `r format(ttest_inv1psquared$all$n_sims, big.mark = " ")` simulations to detect mild miscalibration which is visible in the specifically designed derived quantity as well as when splitting the results based on the standard deviation of the data, see @fig-ttest-inv1psquared. We see similarly mild problems when instead using $\sigma \sim \text{HalfCauchy}(0,1)$, see @fig-ttest-cauchy.


```{r fig-ttest-inv1psquared}
#| fig-cap: Results of SBC when using a $\pi_\text{prior}(\sigma^2) = \frac{1}{1 + \sigma^2}$ when simulating datasets for Bayesian t-test. A) ECDF difference plot for the derived quantity $f(i, y) = (i - \frac{1}{2})(\text{sd}(y) - 1)$ (model_sd_m1). B) and C) Separate binary prediction calibration plots for datasets with high/low standard deviation.

res_ttest_inv1psquared <- load_precomputed_file("cache/ttest_inv1psquared.rds", "ttestBF.qmd")$result


((plot_ecdf_diff(res_ttest_inv1psquared, variables = "model_sd_m1")  + theme(legend.position = "bottom") + labs(tag = "A) SBC") + tag_theme_calib) | (ttest_inv1psquared$high$reliability_diag  + labs(tag = "B) Calibration - sd(y) ‚â• 1") + tag_theme_calib) | 
   (ttest_inv1psquared$low$reliability_diag  + labs(tag = "C) Calibration - sd(y) < 1") + tag_theme_calib))
```

```{r fig-ttest-cauchy}
#| fig-cap: Results of SBC when using a $\sigma \sim \text{HalfCauchy}(0,1)$ when simulating datasets for Bayesian t-test. A) ECDF difference plot for the derived quantity $f(i, y) = (i - \frac{1}{2})(\text{sd}(y) - 1)$ (model_sd_m1). B) and C) Separate binary prediction calibration plots for datasets with high/low standard deviation.

res_ttest_cauchy <- load_precomputed_file("cache/ttest_cauchy.rds", "ttestBF.qmd")$result


((plot_ecdf_diff(res_ttest_cauchy, variables = "model_sd_m1") + theme(legend.position = "bottom") + labs(tag = "A) SBC") + tag_theme_calib) | (ttest_cauchy$high$reliability_diag  + labs(tag = "B) Calibration - sd(y) ‚â• 1") + tag_theme_calib) | 
   (ttest_cauchy$low$reliability_diag  + labs(tag = "C) Calibration - sd(y) < 1") + tag_theme_calib))
```




